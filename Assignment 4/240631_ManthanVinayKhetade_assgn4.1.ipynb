{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05373b1b-ef95-449c-89c7-40069b5588ea",
   "metadata": {},
   "source": [
    "# Assignment 4: Text Classification on TREC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e",
   "metadata": {},
   "source": [
    "We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n",
    "\n",
    "This is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment! \n",
    "\n",
    "Your experimentation should be 4-tiered-\n",
    "\n",
    "i) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n",
    "\n",
    "ii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n",
    "\n",
    "iii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n",
    "\n",
    "iv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n",
    "\n",
    "So applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n",
    "\n",
    "NOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9",
   "metadata": {},
   "source": [
    "### Helper Code to get you started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d08592-c633-4764-a60a-4937fd768cb4",
   "metadata": {},
   "source": [
    "I have added some helper code to show you how to load the TERC dataset and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: How did serfdom develop in and then leave Russia ?\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\", trust_remote_code=True)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(\"Sample Question:\", train_data[0]['text'])\n",
    "print(\"Label:\", train_data[0]['coarse_label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ab43a-d964-4962-aef7-13932f43dd39",
   "metadata": {},
   "source": [
    "### Working with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3be188-8d0c-4d03-bb6f-f222f9c4fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame({\n",
    "    'text': train_data['text'],\n",
    "    'label': train_data['coarse_label']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9631692a-7ad9-4788-b381-79b2539ea1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>What 's the shape of a camel 's spine ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>What type of currency is used in China ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>What is the temperature today ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>What is the temperature for cooking ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>What currency is used in Australia ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     How did serfdom develop in and then leave Russ...      2\n",
       "1      What films featured the character Popeye Doyle ?      1\n",
       "2     How can I find a list of celebrities ' real na...      2\n",
       "3     What fowl grabs the spotlight after the Chines...      1\n",
       "4                       What is the full form of .com ?      0\n",
       "...                                                 ...    ...\n",
       "5447            What 's the shape of a camel 's spine ?      1\n",
       "5448           What type of currency is used in China ?      1\n",
       "5449                    What is the temperature today ?      5\n",
       "5450              What is the temperature for cooking ?      5\n",
       "5451               What currency is used in Australia ?      1\n",
       "\n",
       "[5452 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cc4c8a-ce8f-4ed2-8193-ffbc76e02cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How far is it from Denver to Aspen ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What county is Modesto , California in ?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who was Galileo ?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is an atom ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did Hawaii become a state ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Who was the 22nd President of the US ?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>What is the money they use in Zambia ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>How many feet in a mile ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>What is the birthstone of October ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>What is e-coli ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text  label\n",
       "0        How far is it from Denver to Aspen ?      5\n",
       "1    What county is Modesto , California in ?      4\n",
       "2                           Who was Galileo ?      3\n",
       "3                           What is an atom ?      2\n",
       "4            When did Hawaii become a state ?      5\n",
       "..                                        ...    ...\n",
       "495    Who was the 22nd President of the US ?      3\n",
       "496    What is the money they use in Zambia ?      1\n",
       "497                 How many feet in a mile ?      5\n",
       "498       What is the birthstone of October ?      1\n",
       "499                          What is e-coli ?      2\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    'text': test_data['text'],\n",
    "    'label': test_data['coarse_label']\n",
    "})\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f1c291-6d7b-4d04-8f5a-83d678a21f94",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1627082-e0f5-457e-9d8b-ccfec87ee1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import pos_tag\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1007feb2-e794-419f-83e1-2916cfa3b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(string):\n",
    "    string = re.sub(r'[^A-Za-z0-9\\s]', '', string)\n",
    "    return string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "important_words = {'what', 'when', 'where', 'how', 'why', 'who', 'which', 'whom'}\n",
    "stop_words = stop_words - important_words\n",
    "def remove_sw (string):\n",
    "    string = [word for word in string if word not in stop_words]\n",
    "    return string\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "def pos_tags(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(token_list):\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in token_list]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e68d2c-6543-4d1d-afcf-a4e18c58fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_preprocess(string):\n",
    "    string = string.lower()\n",
    "    string = word_tokenize(string)\n",
    "    string = stem_tokens(string)\n",
    "    string = [remove_special_char(token) for token in string if remove_special_char(token)]\n",
    "    string = remove_sw(string)\n",
    "    return ' '.join(string)\n",
    "\n",
    "\n",
    "def lemma_preprocess(string):\n",
    "    string = string.lower()\n",
    "    string = word_tokenize(string)\n",
    "    tags = pos_tag(string)\n",
    "    string = [lemma.lemmatize(word, pos_tags(pos)) for word, pos in tags]\n",
    "    string = [remove_special_char(token) for token in string if remove_special_char(token)]\n",
    "    string = remove_sw(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1199eb-fc41-454e-8532-16262684f6e6",
   "metadata": {},
   "source": [
    "I have implemented two distinct preprocessing pipelines to suit different types of text vectorization techniques:\n",
    "- `stem_preprocess` : Designed for purely statistical vectorization methods such as Count Vectorizer and TF-IDF Vectorizer.\n",
    "- `lemma_preprocess` : Designed for vectorization methods that capture semantic relationships between words, including Word2Vec models, GloVe embeddings, and similar techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ecebab-639c-41f2-a99c-3a3fb44386e8",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af73105-4b36-4ee6-98d3-8c8dd638e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 20.23 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "df_train['stokens'] = df_train['text'].apply(stem_preprocess)\n",
    "df_test['stokens'] = df_test['text'].apply(stem_preprocess)\n",
    "\n",
    "df_train['ltokens'] = df_train['text'].apply(lemma_preprocess)\n",
    "df_test['ltokens'] = df_test['text'].apply(lemma_preprocess)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db40eeed-9e52-4d9c-8c7a-e85df73968e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>stokens</th>\n",
       "      <th>ltokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>2</td>\n",
       "      <td>how serfdom develop leav russia</td>\n",
       "      <td>[how, serfdom, develop, leave, russia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>1</td>\n",
       "      <td>what film featur charact popey doyl</td>\n",
       "      <td>[what, film, feature, character, popeye, doyle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>2</td>\n",
       "      <td>how find list celebr real name</td>\n",
       "      <td>[how, find, list, celebrity, real, name]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>1</td>\n",
       "      <td>what fowl grab spotlight chines year monkey</td>\n",
       "      <td>[what, fowl, grab, spotlight, chinese, year, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>0</td>\n",
       "      <td>what full form com</td>\n",
       "      <td>[what, full, form, com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>What 's the shape of a camel 's spine ?</td>\n",
       "      <td>1</td>\n",
       "      <td>what shape camel spine</td>\n",
       "      <td>[what, shape, camel, spine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>What type of currency is used in China ?</td>\n",
       "      <td>1</td>\n",
       "      <td>what type currenc use china</td>\n",
       "      <td>[what, type, currency, use, china]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>What is the temperature today ?</td>\n",
       "      <td>5</td>\n",
       "      <td>what temperatur today</td>\n",
       "      <td>[what, temperature, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>What is the temperature for cooking ?</td>\n",
       "      <td>5</td>\n",
       "      <td>what temperatur cook</td>\n",
       "      <td>[what, temperature, cooking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>What currency is used in Australia ?</td>\n",
       "      <td>1</td>\n",
       "      <td>what currenc use australia</td>\n",
       "      <td>[what, currency, use, australia]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     How did serfdom develop in and then leave Russ...      2   \n",
       "1      What films featured the character Popeye Doyle ?      1   \n",
       "2     How can I find a list of celebrities ' real na...      2   \n",
       "3     What fowl grabs the spotlight after the Chines...      1   \n",
       "4                       What is the full form of .com ?      0   \n",
       "...                                                 ...    ...   \n",
       "5447            What 's the shape of a camel 's spine ?      1   \n",
       "5448           What type of currency is used in China ?      1   \n",
       "5449                    What is the temperature today ?      5   \n",
       "5450              What is the temperature for cooking ?      5   \n",
       "5451               What currency is used in Australia ?      1   \n",
       "\n",
       "                                          stokens  \\\n",
       "0                 how serfdom develop leav russia   \n",
       "1             what film featur charact popey doyl   \n",
       "2                  how find list celebr real name   \n",
       "3     what fowl grab spotlight chines year monkey   \n",
       "4                              what full form com   \n",
       "...                                           ...   \n",
       "5447                       what shape camel spine   \n",
       "5448                  what type currenc use china   \n",
       "5449                        what temperatur today   \n",
       "5450                         what temperatur cook   \n",
       "5451                   what currenc use australia   \n",
       "\n",
       "                                                ltokens  \n",
       "0                [how, serfdom, develop, leave, russia]  \n",
       "1       [what, film, feature, character, popeye, doyle]  \n",
       "2              [how, find, list, celebrity, real, name]  \n",
       "3     [what, fowl, grab, spotlight, chinese, year, m...  \n",
       "4                               [what, full, form, com]  \n",
       "...                                                 ...  \n",
       "5447                        [what, shape, camel, spine]  \n",
       "5448                 [what, type, currency, use, china]  \n",
       "5449                         [what, temperature, today]  \n",
       "5450                       [what, temperature, cooking]  \n",
       "5451                   [what, currency, use, australia]  \n",
       "\n",
       "[5452 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc48e52-3b99-4657-83b9-37064a2f1e86",
   "metadata": {},
   "source": [
    "### Stastical vectorisation and model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be781c78-3896-433f-8b83-5e42c7457a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608d6fdd-5d72-4a14-b1f5-d507f6faf615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x_train, x_test, vectorizer):\n",
    "    if vectorizer == 'Count':\n",
    "        vec = CountVectorizer()\n",
    "    elif vectorizer == 'Tfidf': \n",
    "        vec = TfidfVectorizer()\n",
    "        \n",
    "    x_train_vectorized = vec.fit_transform(x_train)\n",
    "    x_test_vectorized = vec.transform(x_test)\n",
    "    vocab_size = len(vec.vocabulary_)\n",
    "    return x_train_vectorized, x_test_vectorized, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53d1e581-ecf0-4992-a74a-da70baf13afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_model(x_train, y_train, x_test, y_test, model, vocab_size = 0):\n",
    "    if model == 'LR':\n",
    "        model = LogisticRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "    elif model == 'SVM':\n",
    "        model = LinearSVC()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "    elif model == 'DT':\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "    elif model == 'NN':\n",
    "        model = keras.Sequential([\n",
    "            layers.Input(shape=(vocab_size,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(6, activation='softmax')\n",
    "            ])\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(x_train.toarray(), y_train, epochs=10, batch_size=32, verbose=0)\n",
    "        y_pred_probs = model.predict(x_test.toarray(), verbose = 0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67004e86-e534-4623-b20a-80ad73aea475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_log(x_train, y_train, x_test, y_test, vectorizers, models):\n",
    "    results = []\n",
    "    for vectorizer in vectorizers:\n",
    "        x_train_vec, x_test_vec, vocab_size = vectorize(x_train, x_test, vectorizer)\n",
    "        for model_type in models:\n",
    "            accuracy = implement_model(x_train_vec, y_train, x_test_vec, y_test, model_type, vocab_size)\n",
    "            results.append({\n",
    "                'Vectorizer': vectorizer,\n",
    "                'Model': model_type,\n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3a381d-4b35-409f-ba4d-e0d100b98412",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_statistic = result_log(df_train['stokens'], df_train['label'], df_test['stokens'], df_test['label'], \n",
    "            vectorizers = ['Count', 'Tfidf'], models = ['LR', 'DT', 'SVM', 'NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049087e5-294d-4edc-be64-ff1f762d592a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Count</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Count</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Count</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vectorizer Model  Accuracy\n",
       "0      Tfidf   SVM     0.862\n",
       "1      Count   SVM     0.854\n",
       "2      Count    LR     0.852\n",
       "3      Tfidf    NN     0.844\n",
       "4      Count    NN     0.840\n",
       "5      Tfidf    LR     0.836\n",
       "6      Tfidf    DT     0.818\n",
       "7      Count    DT     0.816"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcaf43a-f397-4464-9bf1-3b99b1e6fc36",
   "metadata": {},
   "source": [
    "## Semantic vectorization and model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2046f889-21c2-4f16-a53c-cd7691b3eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63c1ddbf-808a-418b-a23e-eecbd900847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MANTHAN KHETADE/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc27565-5585-4138-94df-e2a34eaacd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2063e-01  5.1695e-03 -1.2447e-02 -7.8528e-03 -2.3738e-02 -8.2595e-02\n",
      "  4.5790e-02 -1.5382e-01  6.4550e-02  1.2893e-01  2.7643e-02  1.5958e-02\n",
      "  7.7559e-02  6.0516e-02  1.2737e-01  8.4766e-02  6.3890e-02 -1.7687e-01\n",
      "  4.3017e-02 -1.8031e-02 -3.3041e-02  2.1930e-02 -1.1328e-02  6.6453e-02\n",
      "  1.5826e-01 -2.3008e-02 -4.3616e-03 -2.2379e-02  4.4891e-02  3.0103e-03\n",
      " -1.5565e-02 -7.6785e-02 -9.2186e-02  5.7907e-02 -2.7658e-02  5.4500e-03\n",
      "  1.8975e-02  4.2939e-02  3.4704e-03  4.0449e-02 -4.0245e-03 -1.1594e-01\n",
      " -5.8337e-03  3.2509e-02 -8.6535e-02  7.2000e-02 -2.2299e-02  1.3079e-02\n",
      " -3.9515e-02  6.8996e-02  9.2300e-02 -7.5371e-02  5.9412e-03 -3.4945e-02\n",
      " -3.3417e-02 -9.9982e-02  1.6438e-02  6.3739e-02 -6.2391e-02  7.8285e-04\n",
      " -2.9210e-02 -9.6416e-02  7.2910e-02  4.5905e-02 -8.3387e-02  7.1969e-02\n",
      "  4.0932e-02 -5.6454e-03  1.3709e-01 -1.1793e-01 -7.1011e-02 -7.1963e-02\n",
      "  6.5600e-02 -4.6315e-02 -1.7200e-02  3.4434e-02  4.4218e-02 -9.6354e-03\n",
      " -6.8105e-02  3.0810e-02  1.5424e-02  5.6398e-02  4.4225e-02  8.0547e-02\n",
      " -5.2413e-02 -3.6509e-02  2.6141e-02  2.5574e-02 -3.4346e-02 -4.5879e-02\n",
      " -1.7031e-02  5.1450e-02 -1.2766e-01 -8.6838e-02  1.1084e-02  1.3282e-01\n",
      "  2.0850e-02  7.0881e-02 -5.9277e-03  2.2612e-02  4.8919e-02 -1.2490e-02\n",
      "  1.5460e-01 -6.1251e-03 -8.9369e-02 -2.3707e-01  2.0696e-02 -3.7604e-02\n",
      " -8.3793e-02 -2.5512e-03 -4.0426e-02  1.0575e-01  9.7514e-02  4.4101e-02\n",
      "  4.1732e-02  7.4080e-02  6.3560e-02  3.1801e-02 -1.4961e-02 -4.3675e-03\n",
      " -1.4893e-02  8.6208e-02 -2.0204e-02 -2.0797e-03  7.7648e-02 -1.9620e-03\n",
      "  3.2115e-02 -1.5615e-01 -3.6702e-02  1.2009e-01 -8.0633e-02  4.2894e-02\n",
      " -3.5265e-02  2.2693e-02 -3.3743e-02  1.7573e-02 -7.5089e-02  9.8873e-02\n",
      "  2.7042e-02 -1.7185e-02  1.7489e-02 -1.1096e-01  7.5456e-02 -4.2234e-02\n",
      " -3.7115e-02 -1.2356e-02  1.1243e-02 -4.6907e-02 -5.5681e-02 -6.5216e-02\n",
      "  5.4923e-02  3.7514e-02  5.0259e-02 -7.4453e-02 -2.0440e-02 -8.3293e-02\n",
      " -2.3010e-02 -4.2105e-02 -2.8792e-02 -1.9139e-02  3.6758e-02  7.7620e-02\n",
      " -6.3909e-02 -2.9304e-02  3.1128e-02 -1.2056e-02 -3.0854e-02 -2.3162e-02\n",
      " -4.4762e-02  1.2797e-01 -7.7709e-03 -7.7466e-02 -2.7976e-02  5.1038e-02\n",
      " -5.5217e-02  7.5312e-02  3.4093e-02 -3.4833e-03  9.7360e-03  5.8273e-02\n",
      "  9.3454e-02 -4.3781e-02 -4.5870e-02 -7.3544e-02 -4.1269e-02 -9.1712e-02\n",
      " -1.5840e-01  1.1790e-01  3.4210e-02 -2.4719e-02  6.1251e-02  8.2068e-02\n",
      " -1.1710e-01  2.9949e-02 -7.1442e-02  2.2185e-02 -2.4418e-02 -2.5316e-02\n",
      " -5.3970e-02  1.1615e-01 -1.9979e-01  6.8714e-02 -6.1776e-03 -3.9478e-02\n",
      " -1.8856e-02  7.8819e-02  3.0709e-02 -4.7448e-02 -5.0356e-02 -4.0706e-02\n",
      "  1.4722e-01 -4.6420e-02  1.1976e-05  9.2290e-02 -6.1358e-02  6.0161e-05\n",
      "  1.4491e-02 -2.4847e-02  5.6051e-02  1.9206e-02  3.2446e-02  5.0245e-03\n",
      "  1.9242e-02  1.3482e-01  7.3311e-03 -1.0219e-01  7.6724e-02  9.7512e-02\n",
      " -4.9655e-02 -7.2788e-03 -1.1748e-01 -3.5783e-02 -6.9954e-02 -8.8086e-03\n",
      " -1.5677e-02  6.4489e-02 -7.2463e-02 -5.0428e-03  7.5461e-02 -6.0999e-02\n",
      "  9.2653e-02 -5.3002e-02 -9.8853e-02  4.4468e-02  1.5699e-03  1.0594e-02\n",
      "  5.4306e-02  2.1943e-02 -1.4941e-02 -2.9272e-02  1.0173e-01 -2.7459e-02\n",
      " -1.7016e-02  3.7454e-02  8.5015e-02  8.6834e-02 -7.6342e-02  9.5069e-02\n",
      "  4.6912e-02 -2.2718e-02 -7.9839e-02  6.6125e-02  6.2540e-02  2.5836e-02\n",
      "  2.4580e-02  5.1879e-02 -1.8032e-04  4.8657e-02 -1.1875e-01 -2.4103e-02\n",
      "  1.5130e-03  8.0515e-02 -1.0280e-01 -1.3489e-02  7.1108e-02 -6.0643e-02\n",
      " -2.3006e-02 -9.8232e-03 -8.7159e-02  8.5388e-02  5.3778e-02 -8.4714e-02\n",
      "  5.4218e-02 -4.1406e-02  1.0716e-02  6.9728e-02 -8.9833e-03 -8.0539e-02\n",
      " -3.0566e-02  1.0912e-01 -3.9061e-02 -6.3893e-02 -3.3986e-02 -2.0095e-02\n",
      " -6.0904e-02  1.5957e-02 -1.0371e-02  6.7261e-02 -3.0458e-02 -3.1992e-02]\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "print(fasttext_model['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2c8c70a-b42f-4b7b-b3c8-d8113f387bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "glove_file = r\"C:\\Users\\MANTHAN KHETADE\\Desktop\\glove\\glove.6B\\glove.6B.300d.txt\"\n",
    "glove_model = load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49996ff9-46ac-4a70-885e-37420cde58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_conv(string, model, embedding_dim=300):\n",
    "    vectors = []\n",
    "    for word in string:\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "        else:\n",
    "            vectors.append(np.zeros(embedding_dim))\n",
    "    return vectors       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a729f55-9854-494d-ac47-97e7e8700640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_vectorize(x_train, x_test, vectorizer):\n",
    "    if vectorizer == 'W2V':\n",
    "        model = word2vec_model\n",
    "    elif vectorizer == 'Glove':\n",
    "        model = glove_model\n",
    "    elif vectorizer == 'Fasttext':\n",
    "        model = fasttext_model\n",
    "    x_train_vectorized = x_train.apply(lambda x: vector_conv(x, model))\n",
    "    x_test_vectorized = x_test.apply(lambda x: vector_conv(x, model))\n",
    "    return x_train_vectorized, x_test_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c5d610c-a13e-480e-b505-89f8a92add0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooled(vectors):\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e88fd759-7da9-4bf7-8cdd-587a9bed57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "corpus = [' '.join(tokens) for tokens in df_train['ltokens']]\n",
    "tfidf.fit(corpus)\n",
    "tfidf_dict = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "def tfidf_weighted_pooled(tokens, idf_dict, model):\n",
    "    vectors = []\n",
    "    weights = []\n",
    "    for word in tokens:\n",
    "        if word in model and word in idf_dict:\n",
    "            vectors.append(model[word])\n",
    "            weights.append(idf_dict[word])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(300)\n",
    "    vectors = np.array(vectors)\n",
    "    weights = np.array(weights).reshape(-1, 1)\n",
    "    return np.sum(vectors * weights, axis=0) / np.sum(weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35d4d728-c500-45f1-bd83-dbb48fad6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Masking, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc815c76-5cc7-4455-bb5f-9871a594e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_pooled_vectors(x_train_vec, x_test_vec, y_train, embedding_dim=300, max_sequence_length=30):\n",
    "    x_train_padded = pad_sequences(x_train_vec.tolist(), maxlen=max_sequence_length, dtype='float32', padding='post', truncating='post', value=0.0)\n",
    "    x_test_padded = pad_sequences(x_test_vec.tolist(), maxlen=max_sequence_length, dtype='float32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "    inputs = Input(shape=(max_sequence_length, embedding_dim))\n",
    "    masked = Masking(mask_value=0.0)(inputs)\n",
    "    lstm_out = LSTM(128)(masked)\n",
    "    outputs = Dense(6, activation='softmax')(lstm_out)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    encoder_model = Model(inputs, lstm_out)\n",
    "    x_train_encoded = encoder_model.predict(x_train_padded)\n",
    "    x_test_encoded = encoder_model.predict(x_test_padded)\n",
    "\n",
    "    return x_train_encoded, x_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb349c87-1542-4c67-ac52-3a6d650cfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_model_adv(x_train, y_train, x_test, y_test, model):\n",
    "    input_dim = x_train.shape[1]\n",
    "    if model == 'LR':\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "    elif model == 'SVM':\n",
    "        model = LinearSVC()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "    elif model == 'DT':\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "    elif model == 'NN':\n",
    "        model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(256),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(negative_slope=0.1),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(negative_slope=0.1),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(6, activation='softmax')\n",
    "            ])\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "        y_pred_probs = model.predict(x_test, verbose = 0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "420cca66-56af-49ca-93d1-993dd9db8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_log1(x_train, y_train, x_test, y_test, vectorizers, pooling_types, models):\n",
    "    results = []\n",
    "\n",
    "    for vectorizer in vectorizers:\n",
    "        x_train_vec, x_test_vec = adv_vectorize(x_train, x_test, vectorizer)\n",
    "        \n",
    "        for pooling_type in pooling_types:\n",
    "            if pooling_type == 'Mean':\n",
    "                x_train_pooled = np.vstack(x_train_vec.apply(mean_pooled).values)\n",
    "                x_test_pooled = np.vstack(x_test_vec.apply(mean_pooled).values)\n",
    "\n",
    "            elif pooling_type == 'Tfidf_avg':\n",
    "                if vectorizer == 'W2V':\n",
    "                    model = word2vec_model\n",
    "                elif vectorizer == 'Glove':\n",
    "                    model = glove_model\n",
    "                elif vectorizer == 'Fasttext':\n",
    "                    model = fasttext_model\n",
    "\n",
    "                x_train_pooled = np.vstack(x_train.apply(lambda tokens: tfidf_weighted_pooled(tokens, tfidf_dict, model)).values)\n",
    "                x_test_pooled = np.vstack(x_test.apply(lambda tokens: tfidf_weighted_pooled(tokens, tfidf_dict, model)).values)\n",
    "\n",
    "            elif pooling_type == 'LSTM_encoder':\n",
    "                x_train_pooled, x_test_pooled = get_lstm_pooled_vectors(x_train_vec, x_test_vec, y_train)\n",
    "\n",
    "            for model_type in models:\n",
    "                accuracy = implement_model_adv(x_train_pooled, y_train, x_test_pooled, y_test, model_type)\n",
    "                results.append({\n",
    "                    'Vectorizer': vectorizer,\n",
    "                    'Pooling': pooling_type,\n",
    "                    'Model': model_type,\n",
    "                    'Accuracy': accuracy\n",
    "                })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f85441d-ca05-45dc-97ec-1c62598f8369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    }
   ],
   "source": [
    "result_df = result_log1(df_train['ltokens'], df_train['label'], df_test['ltokens'], df_test['label'],\n",
    "                       vectorizers=['W2V', 'Glove', 'Fasttext'],\n",
    "                       pooling_types=['Mean', 'Tfidf_avg', 'LSTM_encoder'],\n",
    "                       models=['LR', 'SVM', 'DT', 'NN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5182967b-9780-4018-8532-0aecc9f12c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Pooling</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vectorizer       Pooling Model  Accuracy\n",
       "0         W2V  LSTM_encoder    LR     0.900\n",
       "1         W2V  LSTM_encoder   SVM     0.896\n",
       "2    Fasttext  LSTM_encoder    LR     0.888\n",
       "3       Glove  LSTM_encoder    LR     0.884\n",
       "4         W2V  LSTM_encoder    NN     0.884\n",
       "5    Fasttext  LSTM_encoder    NN     0.882\n",
       "6       Glove  LSTM_encoder   SVM     0.882\n",
       "7    Fasttext  LSTM_encoder   SVM     0.878\n",
       "8       Glove  LSTM_encoder    NN     0.870\n",
       "9         W2V          Mean    NN     0.844\n",
       "10   Fasttext          Mean    NN     0.830\n",
       "11        W2V  LSTM_encoder    DT     0.828\n",
       "12   Fasttext  LSTM_encoder    DT     0.824\n",
       "13   Fasttext          Mean   SVM     0.802\n",
       "14        W2V          Mean    LR     0.798\n",
       "15      Glove  LSTM_encoder    DT     0.796\n",
       "16      Glove          Mean    NN     0.790\n",
       "17   Fasttext     Tfidf_avg   SVM     0.784\n",
       "18        W2V          Mean   SVM     0.772\n",
       "19   Fasttext          Mean    LR     0.770\n",
       "20   Fasttext     Tfidf_avg    LR     0.766\n",
       "21      Glove     Tfidf_avg    LR     0.766\n",
       "22        W2V     Tfidf_avg    LR     0.760\n",
       "23        W2V     Tfidf_avg    NN     0.758\n",
       "24      Glove     Tfidf_avg    NN     0.758\n",
       "25      Glove          Mean    LR     0.754\n",
       "26      Glove          Mean   SVM     0.754\n",
       "27      Glove     Tfidf_avg   SVM     0.750\n",
       "28        W2V     Tfidf_avg   SVM     0.742\n",
       "29   Fasttext     Tfidf_avg    NN     0.670\n",
       "30   Fasttext          Mean    DT     0.584\n",
       "31   Fasttext     Tfidf_avg    DT     0.554\n",
       "32        W2V     Tfidf_avg    DT     0.478\n",
       "33      Glove     Tfidf_avg    DT     0.476\n",
       "34      Glove          Mean    DT     0.422\n",
       "35        W2V          Mean    DT     0.402"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445e5df-0f7a-48fe-acc4-79d6cce6d2d5",
   "metadata": {},
   "source": [
    "## Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f38be33-d28b-4687-9252-86e6833860c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_statistic['Pooling'] = None\n",
    "result_df_statistic = result_df_statistic[['Vectorizer', 'Pooling', 'Model', 'Accuracy']]\n",
    "final_report_df = pd.concat([result_df_statistic, result_df], ignore_index=True)\n",
    "final_report_df = final_report_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94bb1be5-7f69-4f8a-841e-564ac8c8ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Pooling</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>None</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Count</td>\n",
       "      <td>None</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count</td>\n",
       "      <td>None</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>None</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Count</td>\n",
       "      <td>None</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>None</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>W2V</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>None</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Count</td>\n",
       "      <td>None</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Glove</td>\n",
       "      <td>LSTM_encoder</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Mean</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Fasttext</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Tfidf_avg</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Glove</td>\n",
       "      <td>Mean</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>W2V</td>\n",
       "      <td>Mean</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vectorizer       Pooling Model  Accuracy\n",
       "0         W2V  LSTM_encoder    LR     0.900\n",
       "1         W2V  LSTM_encoder   SVM     0.896\n",
       "2    Fasttext  LSTM_encoder    LR     0.888\n",
       "3       Glove  LSTM_encoder    LR     0.884\n",
       "4         W2V  LSTM_encoder    NN     0.884\n",
       "5       Glove  LSTM_encoder   SVM     0.882\n",
       "6    Fasttext  LSTM_encoder    NN     0.882\n",
       "7    Fasttext  LSTM_encoder   SVM     0.878\n",
       "8       Glove  LSTM_encoder    NN     0.870\n",
       "9       Tfidf          None   SVM     0.862\n",
       "10      Count          None   SVM     0.854\n",
       "11      Count          None    LR     0.852\n",
       "12      Tfidf          None    NN     0.844\n",
       "13        W2V          Mean    NN     0.844\n",
       "14      Count          None    NN     0.840\n",
       "15      Tfidf          None    LR     0.836\n",
       "16   Fasttext          Mean    NN     0.830\n",
       "17        W2V  LSTM_encoder    DT     0.828\n",
       "18   Fasttext  LSTM_encoder    DT     0.824\n",
       "19      Tfidf          None    DT     0.818\n",
       "20      Count          None    DT     0.816\n",
       "21   Fasttext          Mean   SVM     0.802\n",
       "22        W2V          Mean    LR     0.798\n",
       "23      Glove  LSTM_encoder    DT     0.796\n",
       "24      Glove          Mean    NN     0.790\n",
       "25   Fasttext     Tfidf_avg   SVM     0.784\n",
       "26        W2V          Mean   SVM     0.772\n",
       "27   Fasttext          Mean    LR     0.770\n",
       "28   Fasttext     Tfidf_avg    LR     0.766\n",
       "29      Glove     Tfidf_avg    LR     0.766\n",
       "30        W2V     Tfidf_avg    LR     0.760\n",
       "31        W2V     Tfidf_avg    NN     0.758\n",
       "32      Glove     Tfidf_avg    NN     0.758\n",
       "33      Glove          Mean   SVM     0.754\n",
       "34      Glove          Mean    LR     0.754\n",
       "35      Glove     Tfidf_avg   SVM     0.750\n",
       "36        W2V     Tfidf_avg   SVM     0.742\n",
       "37   Fasttext     Tfidf_avg    NN     0.670\n",
       "38   Fasttext          Mean    DT     0.584\n",
       "39   Fasttext     Tfidf_avg    DT     0.554\n",
       "40        W2V     Tfidf_avg    DT     0.478\n",
       "41      Glove     Tfidf_avg    DT     0.476\n",
       "42      Glove          Mean    DT     0.422\n",
       "43        W2V          Mean    DT     0.402"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10987c2-d44f-4e5a-b359-49d5b6c00e20",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
