{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05373b1b-ef95-449c-89c7-40069b5588ea",
   "metadata": {},
   "source": [
    "# Assignment 4: Text Classification on TREC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e",
   "metadata": {},
   "source": [
    "We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n",
    "\n",
    "This is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment! \n",
    "\n",
    "Your experimentation should be 4-tiered-\n",
    "\n",
    "i) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n",
    "\n",
    "ii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n",
    "\n",
    "iii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n",
    "\n",
    "iv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n",
    "\n",
    "So applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n",
    "\n",
    "NOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9",
   "metadata": {},
   "source": [
    "### Helper Code to get you started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d08592-c633-4764-a60a-4937fd768cb4",
   "metadata": {},
   "source": [
    "I have added some helper code to show you how to load the TERC dataset and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/manan-jain/.cache/huggingface/modules/datasets_modules/datasets/trec/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2 (last modified on Sat Jun  7 21:50:34 2025) since it couldn't be found locally at trec, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: How did serfdom develop in and then leave Russia ?\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\", trust_remote_code=True)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(\"Sample Question:\", train_data[0]['text'])\n",
    "print(\"Label:\", train_data[0]['coarse_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80542bc-6f32-4b6e-b8db-a362dfcc8222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a329cc0-44b6-4b6a-bbca-f8f77fa53cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/manan-\n",
      "[nltk_data]     jain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/manan-\n",
      "[nltk_data]     jain/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def basic_cleanup(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "780bf5a6-3f24-430f-89d1-e8e54210f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "x_train = train_df['text']\n",
    "y_train = train_df['coarse_label']\n",
    "x_test = test_df['text']\n",
    "y_test = test_df['coarse_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff0bf94-960a-42da-b5f8-df4372aade31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "    return \" \".join(basic_cleanup(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b5b118f-6878-429e-aee8-b563cf5e71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    tokens = basic_cleanup(text)\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481e0f08-c5e2-4e15-a1a9-da3a16f4c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    tokens = basic_cleanup(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a56c4c-8888-444d-a053-07210a170cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca9e56e2-b927-4e22-bdf2-a91efaaae594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26573587-45c2-4471-a5cf-161f7f9f7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(preprocess_func, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    x_train_prep = x_train.apply(preprocess_func)\n",
    "    x_test_prep = x_test.apply(preprocess_func)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_vec = vectorizer.fit_transform(x_train_prep)\n",
    "    x_test_vec = vectorizer.transform(x_test_prep)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=2000)\n",
    "    clf.fit(x_train_vec, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test_vec)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8681f0b7-b65e-4bb7-8ebe-10ee7823da69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Stemming/Lemmatizing: 0.756\n",
      "With Stemming: 0.756\n",
      "With Lemmatization: 0.752\n"
     ]
    }
   ],
   "source": [
    "acc_raw = train_and_evaluate(preprocess_raw, x_train, x_test, y_train, y_test)\n",
    "acc_stem = train_and_evaluate(preprocess_stem, x_train, x_test, y_train, y_test)\n",
    "acc_lemma = train_and_evaluate(preprocess_lemma, x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"No Stemming/Lemmatizing:\", acc_raw)\n",
    "print(\"With Stemming:\", acc_stem)\n",
    "print(\"With Lemmatization:\", acc_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3feb344e-f5bb-4260-b856-981a0db43434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.11.3\n",
      "  Using cached scipy-1.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting numpy<1.28.0,>=1.21.6 (from scipy==1.11.3)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached scipy-1.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Installing collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.3\n",
      "    Uninstalling scipy-1.11.3:\n",
      "      Successfully uninstalled scipy-1.11.3\n",
      "Successfully installed numpy-1.26.4 scipy-1.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy==1.11.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccc0ab2-d465-454a-8897-7875f6614415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc91d43c-1a10-44f5-b9fc-9840ab2ca16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7533ae3c-d8dd-432f-a63f-427904ad7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_bow(X_train, X_test):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    return X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99f5ccd6-1caf-4a8a-bece-9ad5d754bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf(X_train, X_test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    return X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39626db-571f-4e92-b980-3a88b7b1dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word2vec(X_train, X_test, sg=0):\n",
    "    tokenized_train = [doc.split() for doc in X_train]\n",
    "    tokenized_test = [doc.split() for doc in X_test]\n",
    "\n",
    "    model = Word2Vec(sentences=tokenized_train, vector_size=10, window=5, min_count=1, sg=sg)\n",
    "\n",
    "    def embed(doc):\n",
    "        vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(model.vector_size)\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    X_train_vec = np.array([embed(doc) for doc in tokenized_train])\n",
    "    X_test_vec = np.array([embed(doc) for doc in tokenized_test])\n",
    "    return X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab24d1-c201-4c42-a3cd-74144680bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea76088b-9cf7-4d26-a8c3-1041d9d71c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_glove(X_train, X_test, glove_path=\"glove.6B.100d.txt\"):\n",
    "    # Load GloVe vectors (make sure to have the file in your working dir)\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            glove[word] = vec\n",
    "\n",
    "    tokenized_train = [doc.split() for doc in X_train]\n",
    "    tokenized_test = [doc.split() for doc in X_test]\n",
    "\n",
    "    def embed(doc):\n",
    "        vectors = [glove[word] for word in doc if word in glove]\n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(100)  # 100d fallback\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    X_train_vec = np.array([embed(doc) for doc in tokenized_train])\n",
    "    X_test_vec = np.array([embed(doc) for doc in tokenized_test])\n",
    "\n",
    "    return X_train_vec, X_test_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a61875c5-c1cb-4bb3-9dd0-b871c72e7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_fasttext(X_train, X_test):\n",
    "    import gensim.downloader as api\n",
    "    import numpy as np\n",
    "\n",
    "    fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "    tokenized_train = [text.split() for text in X_train]\n",
    "    tokenized_test = [text.split() for text in X_test]\n",
    "\n",
    "    def embed(doc):\n",
    "        vectors = [fasttext[word] for word in doc if word in fasttext]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(fasttext.vector_size)\n",
    "\n",
    "    X_train_vec = np.array([embed(doc) for doc in tokenized_train])\n",
    "    X_test_vec = np.array([embed(doc) for doc in tokenized_test])\n",
    "\n",
    "    return X_train_vec, X_test_vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6778e736-a3a9-43e9-941e-3e732be96441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vectorizer(X_train_vec, X_test_vec, y_train, y_test, name):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    preds = model.predict(X_test_vec)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "85cab16a-3afd-4432-b198-e5f314644f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 5452/5452 [00:00<00:00, 26528.12it/s]\n",
      "100%|██████████████████████████████████████| 500/500 [00:00<00:00, 45594.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Accuracy: 0.7520\n",
      "TF-IDF Accuracy: 0.7480\n",
      "Word2Vec CBOW Accuracy: 0.3840\n",
      "Word2Vec Skipgram Accuracy: 0.4400\n",
      "GloVe Accuracy: 0.6040\n",
      "FastText Accuracy: 0.6720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.672"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_prep = [preprocess_lemma(x) for x in tqdm(x_train)]\n",
    "x_test_prep = [preprocess_lemma(x) for x in tqdm(x_test)]\n",
    "\n",
    "# 1. BoW\n",
    "xtr, xte = vectorize_bow(x_train_prep, x_test_prep)\n",
    "evaluate_vectorizer(xtr, xte, y_train, y_test, \"BoW\")\n",
    "\n",
    "# 2. TF-IDF\n",
    "xtr, xte = vectorize_tfidf(x_train_prep, x_test_prep)\n",
    "evaluate_vectorizer(xtr, xte, y_train, y_test, \"TF-IDF\")\n",
    "\n",
    "# 3. Word2Vec CBOW\n",
    "xtr, xte = vectorize_word2vec(x_train_prep, x_test_prep, sg=0)\n",
    "evaluate_vectorizer(xtr, xte, y_train, y_test, \"Word2Vec CBOW\")\n",
    "\n",
    "# 4. Word2Vec Skipgram\n",
    "xtr, xte = vectorize_word2vec(x_train_prep, x_test_prep, sg=1)\n",
    "evaluate_vectorizer(xtr, xte, y_train, y_test, \"Word2Vec Skipgram\")\n",
    "\n",
    "# 5. GloVe\n",
    "xtr, xte = vectorize_glove(x_train_prep, x_test_prep)\n",
    "evaluate_vectorizer(xtr, xte, y_train, y_test, \"GloVe\")\n",
    "\n",
    "# 6. FastText\n",
    "xtr, xte = vectorize_fasttext(x_train_prep, x_test_prep)\n",
    "evaluate_vectorizer(xtr, xte, y_train, y_test, \"FastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85399dda-900c-4d88-9373-820aca168c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Embedding: Word2Vec CBOW + MEAN ---\n",
      "Logistic Regression (mean) Accuracy: 0.2380\n",
      "Decision Tree (mean) Accuracy: 0.4020\n",
      "Random Forest (mean) Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (mean) Accuracy: 0.5460\n",
      "\n",
      "--- Embedding: Word2Vec Skipgram + MEAN ---\n",
      "Logistic Regression (mean) Accuracy: 0.3980\n",
      "Decision Tree (mean) Accuracy: 0.3920\n",
      "Random Forest (mean) Accuracy: 0.5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (mean) Accuracy: 0.5640\n",
      "\n",
      "--- Embedding: Word2Vec CBOW + MAX ---\n",
      "Logistic Regression (max) Accuracy: 0.3720\n",
      "Decision Tree (max) Accuracy: 0.5220\n",
      "Random Forest (max) Accuracy: 0.5980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (max) Accuracy: 0.5000\n",
      "\n",
      "--- Embedding: Word2Vec Skipgram + MAX ---\n",
      "Logistic Regression (max) Accuracy: 0.4220\n",
      "Decision Tree (max) Accuracy: 0.4940\n",
      "Random Forest (max) Accuracy: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (max) Accuracy: 0.5640\n",
      "\n",
      "--- Embedding: Word2Vec CBOW + MIN ---\n",
      "Logistic Regression (min) Accuracy: 0.3760\n",
      "Decision Tree (min) Accuracy: 0.5120\n",
      "Random Forest (min) Accuracy: 0.6160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (min) Accuracy: 0.5200\n",
      "\n",
      "--- Embedding: Word2Vec Skipgram + MIN ---\n",
      "Logistic Regression (min) Accuracy: 0.3920\n",
      "Decision Tree (min) Accuracy: 0.5380\n",
      "Random Forest (min) Accuracy: 0.6160\n",
      "MLP (min) Accuracy: 0.5440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def generate_embeddings(X_train, X_test, sg=0, method=\"mean\"):\n",
    "    tokenized_train = [doc.split() for doc in X_train]\n",
    "    tokenized_test = [doc.split() for doc in X_test]\n",
    "\n",
    "    model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, sg=sg)\n",
    "\n",
    "    def embed(doc):\n",
    "        vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(model.vector_size)\n",
    "        if method == \"mean\":\n",
    "            return np.mean(vectors, axis=0)\n",
    "        elif method == \"max\":\n",
    "            return np.max(vectors, axis=0)\n",
    "        elif method == \"min\":\n",
    "            return np.min(vectors, axis=0)\n",
    "\n",
    "    X_train_vec = np.array([embed(doc) for doc in tokenized_train])\n",
    "    X_test_vec = np.array([embed(doc) for doc in tokenized_test])\n",
    "\n",
    "    return X_train_vec, X_test_vec\n",
    "\n",
    "def evaluate_classifier(X_train_vec, X_test_vec, y_train, y_test, model_name, classifier):\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    preds = classifier.predict(X_test_vec)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"{model_name} Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "# Embedding strategies to try\n",
    "comb_strategies = [\"mean\", \"max\", \"min\"]\n",
    "\n",
    "# Classifiers to try\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"MLP\": MLPClassifier(max_iter=500)\n",
    "}\n",
    "\n",
    "# Loop through strategies and classifiers\n",
    "for strategy in comb_strategies:\n",
    "    print(f\"\\n--- Embedding: Word2Vec CBOW + {strategy.upper()} ---\")\n",
    "    xtr, xte = generate_embeddings(x_train_prep, x_test_prep, sg=0, method=strategy)\n",
    "    for name, clf in models.items():\n",
    "        evaluate_classifier(xtr, xte, y_train, y_test, f\"{name} ({strategy})\", clf)\n",
    "\n",
    "    print(f\"\\n--- Embedding: Word2Vec Skipgram + {strategy.upper()} ---\")\n",
    "    xtr, xte = generate_embeddings(x_train_prep, x_test_prep, sg=1, method=strategy)\n",
    "    for name, clf in models.items():\n",
    "        evaluate_classifier(xtr, xte, y_train, y_test, f\"{name} ({strategy})\", clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "136d559c-55a2-42df-9a45-9bc8e2709616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from tensorflow) (4.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/manan-jain/jupyter_env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.0 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cbeb9dba-56b8-4ce7-8ead-0e6ffec161c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - accuracy: 0.2626 - loss: 1.6796 - val_accuracy: 0.5458 - val_loss: 1.3250\n",
      "Epoch 2/5\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5420 - loss: 1.2526 - val_accuracy: 0.6300 - val_loss: 1.0657\n",
      "Epoch 3/5\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.6347 - loss: 1.0177 - val_accuracy: 0.6557 - val_loss: 0.9504\n",
      "Epoch 4/5\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6746 - loss: 0.9151 - val_accuracy: 0.6850 - val_loss: 0.8688\n",
      "Epoch 5/5\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.6944 - loss: 0.8647 - val_accuracy: 0.7015 - val_loss: 0.8264\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "LSTM Accuracy: 0.6620\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load FastText word vectors\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "embedding_dim = 300\n",
    "\n",
    "# Tokenize the input\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_prep)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train_prep)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test_prep)\n",
    "\n",
    "# Pad sequences\n",
    "maxlen = max(max(len(seq) for seq in x_train_seq), 20)\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=maxlen)\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=maxlen)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in fasttext:\n",
    "        embedding_matrix[i] = fasttext[word]\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(np.unique(y_train_enc)), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(x_train_pad, y_train_enc, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lstm = model.predict(x_test_pad)\n",
    "y_pred_lstm_labels = np.argmax(y_pred_lstm, axis=1)\n",
    "acc_lstm = accuracy_score(y_test_enc, y_pred_lstm_labels)\n",
    "print(f\"LSTM Accuracy: {acc_lstm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5bc313c7-ca38-4f67-b9e2-d42bb1c6efc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running experiments with preprocessing: RAW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing raw: 100%|█████████████████| 5452/5452 [00:00<00:00, 79535.55it/s]\n",
      "Preprocessing raw: 100%|██████████████████| 500/500 [00:00<00:00, 177664.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Vectorizer: BoW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-mean-LogisticRegression: 0.7560\n",
      "BoW-mean-NaiveBayes: 0.5620\n",
      "BoW-mean-SVM: 0.7160\n",
      "BoW-mean-DecisionTree: 0.7160\n",
      "BoW-mean-MLP: 0.7260\n",
      "\n",
      " Vectorizer: TF-IDF\n",
      "TF-IDF-mean-LogisticRegression: 0.7560\n",
      "TF-IDF-mean-NaiveBayes: 0.5680\n",
      "TF-IDF-mean-SVM: 0.7320\n",
      "TF-IDF-mean-DecisionTree: 0.7280\n",
      "TF-IDF-mean-MLP: 0.7100\n",
      "\n",
      " Vectorizer: Word2Vec_CBOW\n",
      "Word2Vec_CBOW-mean-LogisticRegression: 0.2120\n",
      " Error with Word2Vec_CBOW-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "Word2Vec_CBOW-mean-SVM: 0.4440\n",
      "Word2Vec_CBOW-mean-DecisionTree: 0.4120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec_CBOW-mean-MLP: 0.3760\n",
      "\n",
      " Vectorizer: Word2Vec_Skipgram\n",
      "Word2Vec_Skipgram-mean-LogisticRegression: 0.3780\n",
      " Error with Word2Vec_Skipgram-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "Word2Vec_Skipgram-mean-SVM: 0.5760\n",
      "Word2Vec_Skipgram-mean-DecisionTree: 0.4120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec_Skipgram-mean-MLP: 0.5560\n",
      "\n",
      " Vectorizer: GloVe\n",
      "GloVe-mean-LogisticRegression: 0.6080\n",
      " Error with GloVe-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "GloVe-mean-SVM: 0.7320\n",
      "GloVe-mean-DecisionTree: 0.3880\n",
      "GloVe-mean-MLP: 0.6520\n",
      "\n",
      " Vectorizer: FastText\n",
      "FastText-mean-LogisticRegression: 0.6560\n",
      " Error with FastText-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "FastText-mean-SVM: 0.7380\n",
      "FastText-mean-DecisionTree: 0.4100\n",
      "FastText-mean-MLP: 0.6860\n",
      "\n",
      " Running experiments with preprocessing: STEM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing stem: 100%|████████████████| 5452/5452 [00:00<00:00, 10463.42it/s]\n",
      "Preprocessing stem: 100%|██████████████████| 500/500 [00:00<00:00, 16277.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Vectorizer: BoW\n",
      "BoW-mean-LogisticRegression: 0.7560\n",
      "BoW-mean-NaiveBayes: 0.5660\n",
      "BoW-mean-SVM: 0.7280\n",
      "BoW-mean-DecisionTree: 0.7220\n",
      "BoW-mean-MLP: 0.7180\n",
      "\n",
      " Vectorizer: TF-IDF\n",
      "TF-IDF-mean-LogisticRegression: 0.7520\n",
      "TF-IDF-mean-NaiveBayes: 0.5580\n",
      "TF-IDF-mean-SVM: 0.7380\n",
      "TF-IDF-mean-DecisionTree: 0.7020\n",
      "TF-IDF-mean-MLP: 0.6780\n",
      "\n",
      " Vectorizer: Word2Vec_CBOW\n",
      "Word2Vec_CBOW-mean-LogisticRegression: 0.3980\n",
      " Error with Word2Vec_CBOW-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "Word2Vec_CBOW-mean-SVM: 0.5020\n",
      "Word2Vec_CBOW-mean-DecisionTree: 0.4280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec_CBOW-mean-MLP: 0.4400\n",
      "\n",
      " Vectorizer: Word2Vec_Skipgram\n",
      "Word2Vec_Skipgram-mean-LogisticRegression: 0.3860\n",
      " Error with Word2Vec_Skipgram-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "Word2Vec_Skipgram-mean-SVM: 0.4680\n",
      "Word2Vec_Skipgram-mean-DecisionTree: 0.4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec_Skipgram-mean-MLP: 0.6000\n",
      "\n",
      " Vectorizer: GloVe\n",
      "GloVe-mean-LogisticRegression: 0.5460\n",
      " Error with GloVe-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "GloVe-mean-SVM: 0.6780\n",
      "GloVe-mean-DecisionTree: 0.3780\n",
      "GloVe-mean-MLP: 0.5820\n",
      "\n",
      " Vectorizer: FastText\n",
      "FastText-mean-LogisticRegression: 0.6320\n",
      " Error with FastText-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "FastText-mean-SVM: 0.7000\n",
      "FastText-mean-DecisionTree: 0.3920\n",
      "FastText-mean-MLP: 0.6440\n",
      "\n",
      " Running experiments with preprocessing: LEMMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing lemma: 100%|████████████████| 5452/5452 [00:01<00:00, 4821.58it/s]\n",
      "Preprocessing lemma: 100%|█████████████████| 500/500 [00:00<00:00, 17457.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Vectorizer: BoW\n",
      "BoW-mean-LogisticRegression: 0.7520\n",
      "BoW-mean-NaiveBayes: 0.5640\n",
      "BoW-mean-SVM: 0.7060\n",
      "BoW-mean-DecisionTree: 0.7240\n",
      "BoW-mean-MLP: 0.7180\n",
      "\n",
      " Vectorizer: TF-IDF\n",
      "TF-IDF-mean-LogisticRegression: 0.7480\n",
      "TF-IDF-mean-NaiveBayes: 0.5640\n",
      "TF-IDF-mean-SVM: 0.7320\n",
      "TF-IDF-mean-DecisionTree: 0.7300\n",
      "TF-IDF-mean-MLP: 0.6860\n",
      "\n",
      " Vectorizer: Word2Vec_CBOW\n",
      "Word2Vec_CBOW-mean-LogisticRegression: 0.2340\n",
      " Error with Word2Vec_CBOW-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "Word2Vec_CBOW-mean-SVM: 0.4440\n",
      "Word2Vec_CBOW-mean-DecisionTree: 0.4240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec_CBOW-mean-MLP: 0.4140\n",
      "\n",
      " Vectorizer: Word2Vec_Skipgram\n",
      "Word2Vec_Skipgram-mean-LogisticRegression: 0.3900\n",
      " Error with Word2Vec_Skipgram-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "Word2Vec_Skipgram-mean-SVM: 0.5620\n",
      "Word2Vec_Skipgram-mean-DecisionTree: 0.4240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manan-jain/jupyter_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec_Skipgram-mean-MLP: 0.5980\n",
      "\n",
      " Vectorizer: GloVe\n",
      "GloVe-mean-LogisticRegression: 0.6040\n",
      " Error with GloVe-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "GloVe-mean-SVM: 0.7440\n",
      "GloVe-mean-DecisionTree: 0.3780\n",
      "GloVe-mean-MLP: 0.6480\n",
      "\n",
      " Vectorizer: FastText\n",
      "FastText-mean-LogisticRegression: 0.6720\n",
      " Error with FastText-mean-NaiveBayes: Negative values in data passed to MultinomialNB (input X).\n",
      "FastText-mean-SVM: 0.7340\n",
      "FastText-mean-DecisionTree: 0.4020\n",
      "FastText-mean-MLP: 0.6760\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def vectorize_bow(X_train, X_test):\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(X_train), vectorizer.transform(X_test)\n",
    "\n",
    "def vectorize_tfidf(X_train, X_test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    return vectorizer.fit_transform(X_train), vectorizer.transform(X_test)\n",
    "\n",
    "def vectorize_word2vec(X_train, X_test, sg=0):\n",
    "    tokenized_train = [doc.split() for doc in X_train]\n",
    "    tokenized_test = [doc.split() for doc in X_test]\n",
    "    model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, sg=sg)\n",
    "\n",
    "    def embed(doc):\n",
    "        vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "    return np.array([embed(doc) for doc in tokenized_train]), np.array([embed(doc) for doc in tokenized_test])\n",
    "\n",
    "def vectorize_glove(X_train, X_test, glove_path=\"glove.6B.100d.txt\"):\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            glove[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "    def embed(doc):\n",
    "        vectors = [glove[word] for word in doc.split() if word in glove]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "    return np.array([embed(doc) for doc in X_train]), np.array([embed(doc) for doc in X_test])\n",
    "\n",
    "def vectorize_fasttext(X_train, X_test):\n",
    "    fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "    def embed(doc):\n",
    "        vectors = [fasttext[word] for word in doc.split() if word in fasttext]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(fasttext.vector_size)\n",
    "\n",
    "    return np.array([embed(doc) for doc in X_train]), np.array([embed(doc) for doc in X_test])\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=2000),\n",
    "    \"NaiveBayes\": MultinomialNB(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"MLP\": MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n",
    "aggregations = {\n",
    "    \"mean\": lambda x: x  \n",
    "}\n",
    "\n",
    "\n",
    "def run_all_experiments(X_train, X_test, y_train, y_test):\n",
    "    results = []\n",
    "\n",
    "    vectorizers = {\n",
    "        \"BoW\": vectorize_bow,\n",
    "        \"TF-IDF\": vectorize_tfidf,\n",
    "        \"Word2Vec_CBOW\": lambda X_tr, X_te: vectorize_word2vec(X_tr, X_te, sg=0),\n",
    "        \"Word2Vec_Skipgram\": lambda X_tr, X_te: vectorize_word2vec(X_tr, X_te, sg=1),\n",
    "        \"GloVe\": vectorize_glove,\n",
    "        \"FastText\": vectorize_fasttext,\n",
    "    }\n",
    "\n",
    "    for vname, vec_func in vectorizers.items():\n",
    "        print(f\"\\n Vectorizer: {vname}\")\n",
    "        X_tr_vec, X_te_vec = vec_func(X_train, X_test)\n",
    "\n",
    "        for agg_name, agg_func in aggregations.items():\n",
    "            X_tr_agg, X_te_agg = agg_func(X_tr_vec), agg_func(X_te_vec)\n",
    "\n",
    "            for clf_name, clf in classifiers.items():\n",
    "                try:\n",
    "                    clf.fit(X_tr_agg, y_train)\n",
    "                    preds = clf.predict(X_te_agg)\n",
    "                    acc = accuracy_score(y_test, preds)\n",
    "                    print(f\"{vname}-{agg_name}-{clf_name}: {acc:.4f}\")\n",
    "                    results.append({\n",
    "                        \"vectorizer\": vname,\n",
    "                        \"aggregation\": agg_name,\n",
    "                        \"classifier\": clf_name,\n",
    "                        \"accuracy\": acc\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\" Error with {vname}-{agg_name}-{clf_name}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "from tqdm import tqdm\n",
    "preprocessing_methods = {\n",
    "    \"raw\": preprocess_raw,\n",
    "    \"stem\": preprocess_stem,\n",
    "    \"lemma\": preprocess_lemma,\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for prep_name, prep_func in preprocessing_methods.items():\n",
    "    print(f\"\\n Running experiments with preprocessing: {prep_name.upper()}\")\n",
    "\n",
    "    x_train_prep = [prep_func(x) for x in tqdm(x_train, desc=f\"Preprocessing {prep_name}\")]\n",
    "    x_test_prep = [prep_func(x) for x in tqdm(x_test, desc=f\"Preprocessing {prep_name}\")]\n",
    "\n",
    "    results = run_all_experiments(x_train_prep, x_test_prep, y_train, y_test)\n",
    "    for res in results:\n",
    "        res['preprocessing'] = prep_name\n",
    "        all_results.append(res)\n",
    "\n",
    "final_df = pd.DataFrame(all_results)\n",
    "final_df = final_df[['preprocessing', 'vectorizer', 'aggregation', 'classifier', 'accuracy']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88886d08-b6df-469b-b8f3-42936a06fe59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>aggregation</th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>BoW</td>\n",
       "      <td>mean</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw</td>\n",
       "      <td>BoW</td>\n",
       "      <td>mean</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw</td>\n",
       "      <td>BoW</td>\n",
       "      <td>mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw</td>\n",
       "      <td>BoW</td>\n",
       "      <td>mean</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw</td>\n",
       "      <td>BoW</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>lemma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>lemma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>mean</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>lemma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>mean</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>lemma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>mean</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>lemma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   preprocessing vectorizer aggregation          classifier  accuracy\n",
       "0            raw        BoW        mean  LogisticRegression     0.756\n",
       "1            raw        BoW        mean          NaiveBayes     0.562\n",
       "2            raw        BoW        mean                 SVM     0.716\n",
       "3            raw        BoW        mean        DecisionTree     0.716\n",
       "4            raw        BoW        mean                 MLP     0.726\n",
       "..           ...        ...         ...                 ...       ...\n",
       "73         lemma      GloVe        mean                 MLP     0.648\n",
       "74         lemma   FastText        mean  LogisticRegression     0.672\n",
       "75         lemma   FastText        mean                 SVM     0.734\n",
       "76         lemma   FastText        mean        DecisionTree     0.402\n",
       "77         lemma   FastText        mean                 MLP     0.676\n",
       "\n",
       "[78 rows x 5 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cf32af63-6300-4121-8922-94170f1ddb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.756"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['accuracy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e199e40-489b-4ac5-bc7f-6baa4db96f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hence the first combination is the most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff5f98-e2a1-4337-8dbc-6117bf926e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
