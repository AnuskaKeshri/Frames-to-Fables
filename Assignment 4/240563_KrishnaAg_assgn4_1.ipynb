{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VoKisnaHai1102/Frames-to-Fables/blob/main/Assignment%204/240563_KrishnaAg_assgn4_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05373b1b-ef95-449c-89c7-40069b5588ea",
      "metadata": {
        "id": "05373b1b-ef95-449c-89c7-40069b5588ea"
      },
      "source": [
        "# Assignment 4: Text Classification on TREC dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e",
      "metadata": {
        "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e"
      },
      "source": [
        "We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n",
        "\n",
        "This is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment!\n",
        "\n",
        "Your experimentation should be 4-tiered-\n",
        "\n",
        "i) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n",
        "\n",
        "ii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n",
        "\n",
        "iii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n",
        "\n",
        "iv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n",
        "\n",
        "So applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n",
        "\n",
        "NOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9",
      "metadata": {
        "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9"
      },
      "source": [
        "### Helper Code to get you started"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d08592-c633-4764-a60a-4937fd768cb4",
      "metadata": {
        "id": "54d08592-c633-4764-a60a-4937fd768cb4"
      },
      "source": [
        "I have added some helper code to show you how to load the TERC dataset and use it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets nltk scikit-learn gensim tensorflow pandas numpy matplotlib seaborn\n",
        "!pip install fasttext-wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb2a-YowHJRA",
        "outputId": "b3260ce5-7f82-49c4-abbd-c1c39b68cc2d"
      },
      "id": "mb2a-YowHJRA",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: fasttext-wheel in /usr/local/lib/python3.11/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datasets import load_dataset\n",
        "from google.colab import userdata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "dataset = load_dataset(\"trec\", token=hf_token)\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "print(f\"Dataset splits: {list(dataset.keys())}\")\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "print(f\"Features: {train_data.features}\")\n",
        "X_train = [item['text'] for item in train_data]\n",
        "y_train = [item['coarse_label'] for item in train_data]\n",
        "X_test = [item['text'] for item in test_data]\n",
        "y_test = [item['coarse_label'] for item in test_data]\n",
        "\n",
        "\n",
        "coarse_labels = train_data.features['coarse_label'].names\n",
        "print(f\"\\nCoarse label names: {coarse_labels}\")\n",
        "print(f\"Number of classes: {len(coarse_labels)}\")\n",
        "\n",
        "print(\"\\n--- First 3 Examples ---\")\n",
        "for i in range(3):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\" Text: {train_data[i]['text']}\")\n",
        "    print(f\" Coarse: {train_data[i]['coarse_label']} ({coarse_labels[train_data[i]['coarse_label']]})\")\n",
        "    print(f\" Fine: {train_data[i]['fine_label']} ({train_data.features['fine_label'].names[train_data[i]['fine_label']]})\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G17byzjRIpP5",
        "outputId": "6f750ef2-0382-498d-e2c7-b03583436dca"
      },
      "id": "G17byzjRIpP5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits: ['train', 'test']\n",
            "Training samples: 5452\n",
            "Test samples: 500\n",
            "Features: {'text': Value(dtype='string', id=None), 'coarse_label': ClassLabel(names=['ABBR', 'ENTY', 'DESC', 'HUM', 'LOC', 'NUM'], id=None), 'fine_label': ClassLabel(names=['ABBR:abb', 'ABBR:exp', 'ENTY:animal', 'ENTY:body', 'ENTY:color', 'ENTY:cremat', 'ENTY:currency', 'ENTY:dismed', 'ENTY:event', 'ENTY:food', 'ENTY:instru', 'ENTY:lang', 'ENTY:letter', 'ENTY:other', 'ENTY:plant', 'ENTY:product', 'ENTY:religion', 'ENTY:sport', 'ENTY:substance', 'ENTY:symbol', 'ENTY:techmeth', 'ENTY:termeq', 'ENTY:veh', 'ENTY:word', 'DESC:def', 'DESC:desc', 'DESC:manner', 'DESC:reason', 'HUM:gr', 'HUM:ind', 'HUM:title', 'HUM:desc', 'LOC:city', 'LOC:country', 'LOC:mount', 'LOC:other', 'LOC:state', 'NUM:code', 'NUM:count', 'NUM:date', 'NUM:dist', 'NUM:money', 'NUM:ord', 'NUM:other', 'NUM:period', 'NUM:perc', 'NUM:speed', 'NUM:temp', 'NUM:volsize', 'NUM:weight'], id=None)}\n",
            "\n",
            "Coarse label names: ['ABBR', 'ENTY', 'DESC', 'HUM', 'LOC', 'NUM']\n",
            "Number of classes: 6\n",
            "\n",
            "--- First 3 Examples ---\n",
            "Example 1:\n",
            " Text: How did serfdom develop in and then leave Russia ?\n",
            " Coarse: 2 (DESC)\n",
            " Fine: 26 (DESC:manner)\n",
            "\n",
            "Example 2:\n",
            " Text: What films featured the character Popeye Doyle ?\n",
            " Coarse: 1 (ENTY)\n",
            " Fine: 5 (ENTY:cremat)\n",
            "\n",
            "Example 3:\n",
            " Text: How can I find a list of celebrities ' real names ?\n",
            " Coarse: 2 (DESC)\n",
            " Fine: 26 (DESC:manner)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, method='none'):\n",
        "        \"\"\"\n",
        "        method: 'none', 'porter_stem', 'snowball_stem', 'lemmatize'\n",
        "        \"\"\"\n",
        "        self.method = method\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        if method == 'porter_stem':\n",
        "            self.stemmer = PorterStemmer()\n",
        "        elif method == 'snowball_stem':\n",
        "            self.stemmer = SnowballStemmer('english')\n",
        "        elif method == 'lemmatize':\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = text.lower()\n",
        "\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "        if self.method == 'porter_stem':\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "        elif self.method == 'snowball_stem':\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "        elif self.method == 'lemmatize':\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def fit_transform(self, texts):\n",
        "        return [self.preprocess_text(text) for text in texts]\n",
        "\n",
        "    def transform(self, texts):\n",
        "        return [self.preprocess_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "IitLVMPfI_uV"
      },
      "id": "IitLVMPfI_uV",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_preprocessing():\n",
        "    \"\"\"Find the best preprocessing method using BoW + Logistic Regression\"\"\"\n",
        "    preprocessing_methods = ['none', 'porter_stem', 'snowball_stem', 'lemmatize']\n",
        "    results = {}\n",
        "\n",
        "    print(\"Finding best preprocessing method...\")\n",
        "\n",
        "    for method in preprocessing_methods:\n",
        "        print(f\"Testing preprocessing method: {method}\")\n",
        "\n",
        "        preprocessor = TextPreprocessor(method)\n",
        "        X_train_processed = preprocessor.fit_transform(X_train)\n",
        "        X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "        vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "        X_train_vec = vectorizer.fit_transform(X_train_processed)\n",
        "        X_test_vec = vectorizer.transform(X_test_processed)\n",
        "\n",
        "        clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "        clf.fit(X_train_vec, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test_vec)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        results[method] = accuracy\n",
        "\n",
        "        print(f\"Accuracy with {method}: {accuracy:.4f}\")\n",
        "\n",
        "    # Find best method\n",
        "    best_method = max(results, key=results.get)\n",
        "    print(f\"\\nBest preprocessing method: {best_method} with accuracy: {results[best_method]:.4f}\")\n",
        "\n",
        "    return best_method, results\n",
        "\n",
        "# Find best preprocessing method\n",
        "best_preprocessing, preprocessing_results = find_best_preprocessing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acdUhHzWJCt6",
        "outputId": "0337f99c-2f03-431b-d449-84545f6f8bb6"
      },
      "id": "acdUhHzWJCt6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding best preprocessing method...\n",
            "Testing preprocessing method: none\n",
            "Accuracy with none: 0.7500\n",
            "Testing preprocessing method: porter_stem\n",
            "Accuracy with porter_stem: 0.7440\n",
            "Testing preprocessing method: snowball_stem\n",
            "Accuracy with snowball_stem: 0.7460\n",
            "Testing preprocessing method: lemmatize\n",
            "Accuracy with lemmatize: 0.7480\n",
            "\n",
            "Best preprocessing method: none with accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorizationTechniques:\n",
        "    def __init__(self, preprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.vectorizers = {}\n",
        "        self.word2vec_model = None\n",
        "        self.fasttext_model = None\n",
        "\n",
        "    def prepare_data(self, X_train, X_test):\n",
        "        \"\"\"Preprocess the data\"\"\"\n",
        "        self.X_train_processed = self.preprocessor.fit_transform(X_train)\n",
        "        self.X_test_processed = self.preprocessor.transform(X_test)\n",
        "\n",
        "        self.X_train_tokens = [text.split() for text in self.X_train_processed]\n",
        "        self.X_test_tokens = [text.split() for text in self.X_test_processed]\n",
        "\n",
        "        return self.X_train_processed, self.X_test_processed\n",
        "\n",
        "    def bag_of_words(self, max_features=5000):\n",
        "        \"\"\"Bag of Words vectorization\"\"\"\n",
        "        vectorizer = CountVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
        "        X_train_vec = vectorizer.fit_transform(self.X_train_processed)\n",
        "        X_test_vec = vectorizer.transform(self.X_test_processed)\n",
        "        self.vectorizers['bow'] = vectorizer\n",
        "        return X_train_vec.toarray(), X_test_vec.toarray()\n",
        "\n",
        "    def tfidf(self, max_features=5000):\n",
        "        \"\"\"TF-IDF vectorization\"\"\"\n",
        "        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
        "        X_train_vec = vectorizer.fit_transform(self.X_train_processed)\n",
        "        X_test_vec = vectorizer.transform(self.X_test_processed)\n",
        "        self.vectorizers['tfidf'] = vectorizer\n",
        "        return X_train_vec.toarray(), X_test_vec.toarray()\n",
        "\n",
        "    def word2vec_cbow(self, vector_size=100, window=5, min_count=2):\n",
        "        \"\"\"Word2Vec CBOW model\"\"\"\n",
        "        model = Word2Vec(\n",
        "            sentences=self.X_train_tokens,\n",
        "            vector_size=vector_size,\n",
        "            window=window,\n",
        "            min_count=min_count,\n",
        "            sg=0,  # CBOW\n",
        "            seed=42,\n",
        "            workers=4\n",
        "        )\n",
        "        self.word2vec_model = model\n",
        "        return model\n",
        "\n",
        "    def word2vec_skipgram(self, vector_size=100, window=5, min_count=2):\n",
        "        \"\"\"Word2Vec Skip-gram model\"\"\"\n",
        "        model = Word2Vec(\n",
        "            sentences=self.X_train_tokens,\n",
        "            vector_size=vector_size,\n",
        "            window=window,\n",
        "            min_count=min_count,\n",
        "            sg=1,  # Skip-gram\n",
        "            seed=42,\n",
        "            workers=4\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def build_fasttext_model(self, vector_size=100, window=5, min_count=2):\n",
        "        \"\"\"FastText model\"\"\"\n",
        "        model = FastText(\n",
        "            sentences=self.X_train_tokens,\n",
        "            vector_size=vector_size,\n",
        "            window=window,\n",
        "            min_count=min_count,\n",
        "            seed=42,\n",
        "            workers=4\n",
        "        )\n",
        "        self.fasttext_model = model\n",
        "        return model\n",
        "\n",
        "    def get_glove_embeddings(self, vector_size=100):\n",
        "        \"\"\"Download and use GloVe embeddings (simplified version)\"\"\"\n",
        "        vocab = set()\n",
        "        for tokens in self.X_train_tokens:\n",
        "            vocab.update(tokens)\n",
        "\n",
        "        vocab_size = len(vocab)\n",
        "        embedding_matrix = np.random.normal(size=(vocab_size, vector_size))\n",
        "        word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "        return embedding_matrix, word_to_idx\n",
        "\n",
        "class CombinationStrategies:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def mean_pooling(self, embeddings):\n",
        "        \"\"\"Average word embeddings\"\"\"\n",
        "        return np.mean(embeddings, axis=0) if len(embeddings) > 0 else np.zeros(embeddings.shape[1] if len(embeddings.shape) > 1 else 100)\n",
        "\n",
        "    def max_pooling(self, embeddings):\n",
        "        \"\"\"Max pooling of word embeddings\"\"\"\n",
        "        return np.max(embeddings, axis=0) if len(embeddings) > 0 else np.zeros(embeddings.shape[1] if len(embeddings.shape) > 1 else 100)\n",
        "\n",
        "    def get_sentence_embeddings(self, tokenized_sentences, model, strategy='mean'):\n",
        "        \"\"\"Convert tokenized sentences to sentence embeddings\"\"\"\n",
        "        sentence_embeddings = []\n",
        "\n",
        "        for tokens in tokenized_sentences:\n",
        "            word_embeddings = []\n",
        "            for token in tokens:\n",
        "                if token in model.wv:\n",
        "                    word_embeddings.append(model.wv[token])\n",
        "\n",
        "            if word_embeddings:\n",
        "                word_embeddings = np.array(word_embeddings)\n",
        "                if strategy == 'mean':\n",
        "                    sentence_emb = self.mean_pooling(word_embeddings)\n",
        "                elif strategy == 'max':\n",
        "                    sentence_emb = self.max_pooling(word_embeddings)\n",
        "                else:\n",
        "                    sentence_emb = np.mean(word_embeddings, axis=0)  # default to mean\n",
        "            else:\n",
        "                sentence_emb = np.zeros(model.vector_size)\n",
        "\n",
        "            sentence_embeddings.append(sentence_emb)\n",
        "\n",
        "        return np.array(sentence_embeddings)\n",
        "\n",
        "    def lstm_combination(self, tokenized_sentences, model, max_len=50):\n",
        "        \"\"\"Use LSTM to combine word embeddings\"\"\"\n",
        "        vocab = set()\n",
        "        for tokens in tokenized_sentences:\n",
        "            vocab.update(tokens)\n",
        "\n",
        "        word_to_idx = {word: i+1 for i, word in enumerate(vocab)}  # 0 reserved for padding\n",
        "        vocab_size = len(word_to_idx) + 1\n",
        "\n",
        "        sequences = []\n",
        "        for tokens in tokenized_sentences:\n",
        "            seq = [word_to_idx.get(token, 0) for token in tokens]\n",
        "            sequences.append(seq)\n",
        "\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "\n",
        "        embedding_matrix = np.zeros((vocab_size, model.vector_size))\n",
        "        for word, idx in word_to_idx.items():\n",
        "            if word in model.wv:\n",
        "                embedding_matrix[idx] = model.wv[word]\n",
        "\n",
        "        return padded_sequences, embedding_matrix, vocab_size\n",
        "\n",
        "\n",
        "class MLModels:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "\n",
        "    def logistic_regression(self):\n",
        "        return LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    def decision_tree(self):\n",
        "        return DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "    def random_forest(self):\n",
        "        return RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    def svm(self):\n",
        "        return SVC(random_state=42, kernel='rbf')\n",
        "\n",
        "    def mlp_classifier(self):\n",
        "        return MLPClassifier(hidden_layer_sizes=(128, 64), random_state=42, max_iter=500)\n",
        "\n",
        "    def lstm_model(self, vocab_size, embedding_matrix, max_len=50, num_classes=6):\n",
        "        \"\"\"Create LSTM model for sequence classification\"\"\"\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, embedding_matrix.shape[1],\n",
        "                     weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "            LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model"
      ],
      "metadata": {
        "id": "Ti9jKmZ4KBXT"
      },
      "id": "Ti9jKmZ4KBXT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_comprehensive_experiment():\n",
        "    \"\"\"Run all combinations of experiments\"\"\"\n",
        "\n",
        "    preprocessor = TextPreprocessor(best_preprocessing)\n",
        "    vectorizer = VectorizationTechniques(preprocessor)\n",
        "    combiner = CombinationStrategies()\n",
        "    ml_models = MLModels()\n",
        "\n",
        "    X_train_processed, X_test_processed = vectorizer.prepare_data(X_train, X_test)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(\"Starting comprehensive experimentation...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    vectorization_methods = [\n",
        "        'bow', 'tfidf', 'word2vec_cbow', 'word2vec_skipgram', 'fasttext'\n",
        "    ]\n",
        "\n",
        "    combination_strategies = ['mean', 'max', 'lstm']\n",
        "\n",
        "    ml_model_names = ['logistic_regression', 'decision_tree', 'random_forest', 'svm', 'mlp']\n",
        "\n",
        "    print(\"Training embedding models...\")\n",
        "    w2v_cbow_model = vectorizer.word2vec_cbow()\n",
        "    w2v_skipgram_model = vectorizer.word2vec_skipgram()\n",
        "    fasttext_model = vectorizer.build_fasttext_model()\n",
        "\n",
        "    embedding_models = {\n",
        "        'word2vec_cbow': w2v_cbow_model,\n",
        "        'word2vec_skipgram': w2v_skipgram_model,\n",
        "        'fasttext': fasttext_model\n",
        "    }\n",
        "\n",
        "    exp_count = 0\n",
        "    total_experiments = len(vectorization_methods) * len(ml_model_names) + \\\n",
        "                       len([v for v in vectorization_methods if v in embedding_models]) * len(combination_strategies) * len(ml_model_names)\n",
        "\n",
        "    # 1. Traditional vectorization methods (BoW, TF-IDF)\n",
        "    for vec_method in ['bow', 'tfidf']:\n",
        "        print(f\"\\nTesting {vec_method.upper()} vectorization...\")\n",
        "\n",
        "        if vec_method == 'bow':\n",
        "            X_train_vec, X_test_vec = vectorizer.bag_of_words()\n",
        "        else:  # tfidf\n",
        "            X_train_vec, X_test_vec = vectorizer.tfidf()\n",
        "\n",
        "        for model_name in ml_model_names:\n",
        "            exp_count += 1\n",
        "            print(f\"[{exp_count}/{total_experiments}] {vec_method} + {model_name}\")\n",
        "\n",
        "            try:\n",
        "                # Get model\n",
        "                if model_name == 'logistic_regression':\n",
        "                    model = ml_models.logistic_regression()\n",
        "                elif model_name == 'decision_tree':\n",
        "                    model = ml_models.decision_tree()\n",
        "                elif model_name == 'random_forest':\n",
        "                    model = ml_models.random_forest()\n",
        "                elif model_name == 'svm':\n",
        "                    model = ml_models.svm()\n",
        "                elif model_name == 'mlp':\n",
        "                    model = ml_models.mlp_classifier()\n",
        "\n",
        "                # Train and evaluate\n",
        "                start_time = time.time()\n",
        "                model.fit(X_train_vec, y_train)\n",
        "                y_pred = model.predict(X_test_vec)\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                train_time = time.time() - start_time\n",
        "\n",
        "                results.append({\n",
        "                    'Preprocessing': best_preprocessing,\n",
        "                    'Vectorization': vec_method.upper(),\n",
        "                    'Combination': 'N/A',\n",
        "                    'ML_Model': model_name,\n",
        "                    'Accuracy': accuracy,\n",
        "                    'Train_Time': train_time\n",
        "                })\n",
        "\n",
        "                print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {str(e)}\")\n",
        "                results.append({\n",
        "                    'Preprocessing': best_preprocessing,\n",
        "                    'Vectorization': vec_method.upper(),\n",
        "                    'Combination': 'N/A',\n",
        "                    'ML_Model': model_name,\n",
        "                    'Accuracy': 0.0,\n",
        "                    'Train_Time': 0.0\n",
        "                })\n",
        "\n",
        "    # 2. Embedding methods with combination strategies\n",
        "    for vec_method in ['word2vec_cbow', 'word2vec_skipgram', 'fasttext']:\n",
        "        print(f\"\\nTesting {vec_method.upper()} embeddings...\")\n",
        "        model = embedding_models[vec_method]\n",
        "\n",
        "        for combination in combination_strategies:\n",
        "            if combination == 'lstm':\n",
        "                # Special handling for LSTM\n",
        "                print(f\"  Testing {combination.upper()} combination with LSTM classifier...\")\n",
        "\n",
        "                try:\n",
        "                    # Prepare data for LSTM\n",
        "                    X_train_seq, embedding_matrix, vocab_size = combiner.lstm_combination(\n",
        "                        vectorizer.X_train_tokens, model\n",
        "                    )\n",
        "                    X_test_seq, _, _ = combiner.lstm_combination(\n",
        "                        vectorizer.X_test_tokens, model\n",
        "                    )\n",
        "\n",
        "                    # Convert labels to categorical\n",
        "                    y_train_cat = to_categorical(y_train, num_classes=6)\n",
        "                    y_test_cat = to_categorical(y_test, num_classes=6)\n",
        "\n",
        "                    # Create and train LSTM model\n",
        "                    lstm_model = ml_models.lstm_model(vocab_size, embedding_matrix)\n",
        "\n",
        "                    start_time = time.time()\n",
        "                    lstm_model.fit(X_train_seq, y_train_cat, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "                    # Predict\n",
        "                    y_pred_proba = lstm_model.predict(X_test_seq, verbose=0)\n",
        "                    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    train_time = time.time() - start_time\n",
        "\n",
        "                    exp_count += 1\n",
        "                    print(f\"[{exp_count}] {vec_method} + {combination} + LSTM: {accuracy:.4f}\")\n",
        "\n",
        "                    results.append({\n",
        "                        'Preprocessing': best_preprocessing,\n",
        "                        'Vectorization': vec_method,\n",
        "                        'Combination': combination,\n",
        "                        'ML_Model': 'LSTM',\n",
        "                        'Accuracy': accuracy,\n",
        "                        'Train_Time': train_time\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error with LSTM: {str(e)}\")\n",
        "                    results.append({\n",
        "                        'Preprocessing': best_preprocessing,\n",
        "                        'Vectorization': vec_method,\n",
        "                        'Combination': combination,\n",
        "                        'ML_Model': 'LSTM',\n",
        "                        'Accuracy': 0.0,\n",
        "                        'Train_Time': 0.0\n",
        "                    })\n",
        "\n",
        "            else:\n",
        "                # Traditional combination strategies\n",
        "                print(f\"  Testing {combination.upper()} combination...\")\n",
        "\n",
        "                # Get sentence embeddings\n",
        "                X_train_emb = combiner.get_sentence_embeddings(\n",
        "                    vectorizer.X_train_tokens, model, combination\n",
        "                )\n",
        "                X_test_emb = combiner.get_sentence_embeddings(\n",
        "                    vectorizer.X_test_tokens, model, combination\n",
        "                )\n",
        "\n",
        "                for model_name in ml_model_names:\n",
        "                    exp_count += 1\n",
        "                    print(f\"[{exp_count}] {vec_method} + {combination} + {model_name}\")\n",
        "\n",
        "                    try:\n",
        "                        # Get model\n",
        "                        if model_name == 'logistic_regression':\n",
        "                            ml_model = ml_models.logistic_regression()\n",
        "                        elif model_name == 'decision_tree':\n",
        "                            ml_model = ml_models.decision_tree()\n",
        "                        elif model_name == 'random_forest':\n",
        "                            ml_model = ml_models.random_forest()\n",
        "                        elif model_name == 'svm':\n",
        "                            ml_model = ml_models.svm()\n",
        "                        elif model_name == 'mlp':\n",
        "                            ml_model = ml_models.mlp_classifier()\n",
        "\n",
        "                        # Train and evaluate\n",
        "                        start_time = time.time()\n",
        "                        ml_model.fit(X_train_emb, y_train)\n",
        "                        y_pred = ml_model.predict(X_test_emb)\n",
        "                        accuracy = accuracy_score(y_test, y_pred)\n",
        "                        train_time = time.time() - start_time\n",
        "\n",
        "                        results.append({\n",
        "                            'Preprocessing': best_preprocessing,\n",
        "                            'Vectorization': vec_method,\n",
        "                            'Combination': combination,\n",
        "                            'ML_Model': model_name,\n",
        "                            'Accuracy': accuracy,\n",
        "                            'Train_Time': train_time\n",
        "                        })\n",
        "\n",
        "                        print(f\"    Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"    Error: {str(e)}\")\n",
        "                        results.append({\n",
        "                            'Preprocessing': best_preprocessing,\n",
        "                            'Vectorization': vec_method,\n",
        "                            'Combination': combination,\n",
        "                            'ML_Model': model_name,\n",
        "                            'Accuracy': 0.0,\n",
        "                            'Train_Time': 0.0\n",
        "                        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KnALI0pZKQcg"
      },
      "id": "KnALI0pZKQcg",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "experiment_results = run_comprehensive_experiment()\n",
        "\n",
        "results_df = pd.DataFrame(experiment_results)\n",
        "\n",
        "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPREHENSIVE EXPERIMENT RESULTS\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "best_result = results_df.iloc[0]\n",
        "print(f\"\\n BEST PERFORMING MODEL:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Preprocessing: {best_result['Preprocessing']}\")\n",
        "print(f\"Vectorization: {best_result['Vectorization']}\")\n",
        "print(f\"Combination Strategy: {best_result['Combination']}\")\n",
        "print(f\"ML Model: {best_result['ML_Model']}\")\n",
        "print(f\"Accuracy: {best_result['Accuracy']:.4f}\")\n",
        "print(f\"Training Time: {best_result['Train_Time']:.2f} seconds\")\n",
        "\n",
        "print(f\"\\n TOP 10 PERFORMING COMBINATIONS:\")\n",
        "print(\"=\"*50)\n",
        "top_10 = results_df.head(10)\n",
        "for i, row in top_10.iterrows():\n",
        "    print(f\"{i+1:2d}. {row['Vectorization']:15s} + {row['Combination']:8s} + {row['ML_Model']:18s} = {row['Accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiIPIFeU40il",
        "outputId": "78d85aac-0e90-4112-edf7-c3ef3cecadcd"
      },
      "id": "yiIPIFeU40il",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive experimentation...\n",
            "================================================================================\n",
            "Training embedding models...\n",
            "\n",
            "Testing BOW vectorization...\n",
            "[1/70] bow + logistic_regression\n",
            "  Accuracy: 0.7500\n",
            "[2/70] bow + decision_tree\n",
            "  Accuracy: 0.7140\n",
            "[3/70] bow + random_forest\n",
            "  Accuracy: 0.7200\n",
            "[4/70] bow + svm\n",
            "  Accuracy: 0.7080\n",
            "[5/70] bow + mlp\n",
            "  Accuracy: 0.7000\n",
            "\n",
            "Testing TFIDF vectorization...\n",
            "[6/70] tfidf + logistic_regression\n",
            "  Accuracy: 0.7500\n",
            "[7/70] tfidf + decision_tree\n",
            "  Accuracy: 0.7340\n",
            "[8/70] tfidf + random_forest\n",
            "  Accuracy: 0.7320\n",
            "[9/70] tfidf + svm\n",
            "  Accuracy: 0.7260\n",
            "[10/70] tfidf + mlp\n",
            "  Accuracy: 0.7100\n",
            "\n",
            "Testing WORD2VEC_CBOW embeddings...\n",
            "  Testing MEAN combination...\n",
            "[11] word2vec_cbow + mean + logistic_regression\n",
            "    Accuracy: 0.4180\n",
            "[12] word2vec_cbow + mean + decision_tree\n",
            "    Accuracy: 0.5060\n",
            "[13] word2vec_cbow + mean + random_forest\n",
            "    Accuracy: 0.5860\n",
            "[14] word2vec_cbow + mean + svm\n",
            "    Accuracy: 0.6360\n",
            "[15] word2vec_cbow + mean + mlp\n",
            "    Accuracy: 0.5940\n",
            "  Testing MAX combination...\n",
            "[16] word2vec_cbow + max + logistic_regression\n",
            "    Accuracy: 0.3940\n",
            "[17] word2vec_cbow + max + decision_tree\n",
            "    Accuracy: 0.5920\n",
            "[18] word2vec_cbow + max + random_forest\n",
            "    Accuracy: 0.6340\n",
            "[19] word2vec_cbow + max + svm\n",
            "    Accuracy: 0.6060\n",
            "[20] word2vec_cbow + max + mlp\n",
            "    Accuracy: 0.5540\n",
            "  Testing LSTM combination with LSTM classifier...\n",
            "[21] word2vec_cbow + lstm + LSTM: 0.1300\n",
            "\n",
            "Testing WORD2VEC_SKIPGRAM embeddings...\n",
            "  Testing MEAN combination...\n",
            "[22] word2vec_skipgram + mean + logistic_regression\n",
            "    Accuracy: 0.3800\n",
            "[23] word2vec_skipgram + mean + decision_tree\n",
            "    Accuracy: 0.4720\n",
            "[24] word2vec_skipgram + mean + random_forest\n",
            "    Accuracy: 0.5700\n",
            "[25] word2vec_skipgram + mean + svm\n",
            "    Accuracy: 0.5480\n",
            "[26] word2vec_skipgram + mean + mlp\n",
            "    Accuracy: 0.6260\n",
            "  Testing MAX combination...\n",
            "[27] word2vec_skipgram + max + logistic_regression\n",
            "    Accuracy: 0.3900\n",
            "[28] word2vec_skipgram + max + decision_tree\n",
            "    Accuracy: 0.5920\n",
            "[29] word2vec_skipgram + max + random_forest\n",
            "    Accuracy: 0.6500\n",
            "[30] word2vec_skipgram + max + svm\n",
            "    Accuracy: 0.5400\n",
            "[31] word2vec_skipgram + max + mlp\n",
            "    Accuracy: 0.5720\n",
            "  Testing LSTM combination with LSTM classifier...\n",
            "[32] word2vec_skipgram + lstm + LSTM: 0.1300\n",
            "\n",
            "Testing FASTTEXT embeddings...\n",
            "  Testing MEAN combination...\n",
            "[33] fasttext + mean + logistic_regression\n",
            "    Accuracy: 0.3260\n",
            "[34] fasttext + mean + decision_tree\n",
            "    Accuracy: 0.3060\n",
            "[35] fasttext + mean + random_forest\n",
            "    Accuracy: 0.4620\n",
            "[36] fasttext + mean + svm\n",
            "    Accuracy: 0.3640\n",
            "[37] fasttext + mean + mlp\n",
            "    Accuracy: 0.4340\n",
            "  Testing MAX combination...\n",
            "[38] fasttext + max + logistic_regression\n",
            "    Accuracy: 0.3540\n",
            "[39] fasttext + max + decision_tree\n",
            "    Accuracy: 0.5020\n",
            "[40] fasttext + max + random_forest\n",
            "    Accuracy: 0.6360\n",
            "[41] fasttext + max + svm\n",
            "    Accuracy: 0.3740\n",
            "[42] fasttext + max + mlp\n",
            "    Accuracy: 0.4880\n",
            "  Testing LSTM combination with LSTM classifier...\n",
            "[43] fasttext + lstm + LSTM: 0.1300\n",
            "\n",
            "====================================================================================================\n",
            "COMPREHENSIVE EXPERIMENT RESULTS\n",
            "====================================================================================================\n",
            "Preprocessing     Vectorization Combination            ML_Model  Accuracy  Train_Time\n",
            "         none               BOW         N/A logistic_regression     0.750   18.880795\n",
            "         none             TFIDF         N/A logistic_regression     0.750   14.317547\n",
            "         none             TFIDF         N/A       decision_tree     0.734   47.673000\n",
            "         none             TFIDF         N/A       random_forest     0.732   58.131318\n",
            "         none             TFIDF         N/A                 svm     0.726  124.609688\n",
            "         none               BOW         N/A       random_forest     0.720   56.854290\n",
            "         none               BOW         N/A       decision_tree     0.714   43.784461\n",
            "         none             TFIDF         N/A                 mlp     0.710  102.149981\n",
            "         none               BOW         N/A                 svm     0.708  209.382121\n",
            "         none               BOW         N/A                 mlp     0.700  108.021433\n",
            "         none word2vec_skipgram         max       random_forest     0.650    7.523548\n",
            "         none     word2vec_cbow        mean                 svm     0.636    3.755241\n",
            "         none          fasttext         max       random_forest     0.636    7.060700\n",
            "         none     word2vec_cbow         max       random_forest     0.634    7.998532\n",
            "         none word2vec_skipgram        mean                 mlp     0.626   54.994838\n",
            "         none     word2vec_cbow         max                 svm     0.606    3.791538\n",
            "         none     word2vec_cbow        mean                 mlp     0.594   51.667613\n",
            "         none     word2vec_cbow         max       decision_tree     0.592    1.048632\n",
            "         none word2vec_skipgram         max       decision_tree     0.592    0.987643\n",
            "         none     word2vec_cbow        mean       random_forest     0.586    9.139316\n",
            "         none word2vec_skipgram         max                 mlp     0.572   51.327758\n",
            "         none word2vec_skipgram        mean       random_forest     0.570    9.802378\n",
            "         none     word2vec_cbow         max                 mlp     0.554   51.120943\n",
            "         none word2vec_skipgram        mean                 svm     0.548    5.445760\n",
            "         none word2vec_skipgram         max                 svm     0.540    3.632816\n",
            "         none     word2vec_cbow        mean       decision_tree     0.506    1.282978\n",
            "         none          fasttext         max       decision_tree     0.502    1.545230\n",
            "         none          fasttext         max                 mlp     0.488   35.386017\n",
            "         none word2vec_skipgram        mean       decision_tree     0.472    1.390635\n",
            "         none          fasttext        mean       random_forest     0.462    9.934503\n",
            "         none          fasttext        mean                 mlp     0.434   49.974686\n",
            "         none     word2vec_cbow        mean logistic_regression     0.418    0.080256\n",
            "         none     word2vec_cbow         max logistic_regression     0.394    0.122721\n",
            "         none word2vec_skipgram         max logistic_regression     0.390    0.282729\n",
            "         none word2vec_skipgram        mean logistic_regression     0.380    0.188286\n",
            "         none          fasttext         max                 svm     0.374    4.893431\n",
            "         none          fasttext        mean                 svm     0.364    5.376424\n",
            "         none          fasttext         max logistic_regression     0.354    0.227351\n",
            "         none          fasttext        mean logistic_regression     0.326    0.153643\n",
            "         none          fasttext        mean       decision_tree     0.306    1.772614\n",
            "         none word2vec_skipgram        lstm                LSTM     0.130  108.359595\n",
            "         none     word2vec_cbow        lstm                LSTM     0.130  104.555807\n",
            "         none          fasttext        lstm                LSTM     0.130  106.766500\n",
            "\n",
            " BEST PERFORMING MODEL:\n",
            "==================================================\n",
            "Preprocessing: none\n",
            "Vectorization: BOW\n",
            "Combination Strategy: N/A\n",
            "ML Model: logistic_regression\n",
            "Accuracy: 0.7500\n",
            "Training Time: 18.88 seconds\n",
            "\n",
            " TOP 10 PERFORMING COMBINATIONS:\n",
            "==================================================\n",
            " 1. BOW             + N/A      + logistic_regression = 0.7500\n",
            " 6. TFIDF           + N/A      + logistic_regression = 0.7500\n",
            " 7. TFIDF           + N/A      + decision_tree      = 0.7340\n",
            " 8. TFIDF           + N/A      + random_forest      = 0.7320\n",
            " 9. TFIDF           + N/A      + svm                = 0.7260\n",
            " 3. BOW             + N/A      + random_forest      = 0.7200\n",
            " 2. BOW             + N/A      + decision_tree      = 0.7140\n",
            "10. TFIDF           + N/A      + mlp                = 0.7100\n",
            " 4. BOW             + N/A      + svm                = 0.7080\n",
            " 5. BOW             + N/A      + mlp                = 0.7000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}