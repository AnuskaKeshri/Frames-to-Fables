{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05373b1b-ef95-449c-89c7-40069b5588ea",
   "metadata": {},
   "source": [
    "# Assignment 4: Text Classification on TREC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e",
   "metadata": {},
   "source": [
    "We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n",
    "\n",
    "This is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment! \n",
    "\n",
    "Your experimentation should be 4-tiered-\n",
    "\n",
    "i) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n",
    "\n",
    "ii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n",
    "\n",
    "iii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n",
    "\n",
    "iv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n",
    "\n",
    "So applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n",
    "\n",
    "NOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9",
   "metadata": {},
   "source": [
    "### Helper Code to get you started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d08592-c633-4764-a60a-4937fd768cb4",
   "metadata": {},
   "source": [
    "I have added some helper code to show you how to load the TERC dataset and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: How did serfdom develop in and then leave Russia ?\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\", trust_remote_code=True)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "\n",
    "print(\"Sample Question:\", train_data[0]['text'])\n",
    "print(\"Label:\", train_data[0]['coarse_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498abecd-a9b3-4bdb-921a-272a98087477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbaee886-6127-46c2-901c-8b79579c5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stemming(texts):\n",
    "    return [\" \".join([stemmer.stem(word) \n",
    "                      for word in word_tokenize(text)]) \n",
    "            for text in texts]\n",
    "        \n",
    "def lemmetizing(texts):\n",
    "    return [\" \".join([lemmatizer.lemmatize(word.lower()) \n",
    "                      for word in word_tokenize(text)]) \n",
    "            for text in texts]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdf8ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how far is it from denver to aspen ?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [item[\"text\"] for item in train_data]\n",
    "labels = [item[\"coarse_label\"] for item in train_data]\n",
    "lemme_text =lemmetizing(texts)\n",
    "stemm_text=stemming(texts)\n",
    "test_texts=[item[\"text\"] for item in test_data]\n",
    "test_labels = [item[\"coarse_label\"] for item in test_data]\n",
    "test_lemme_text =lemmetizing(test_texts)\n",
    "test_stemm_text = stemming(test_texts)\n",
    "test_lemme_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0f4ac",
   "metadata": {},
   "source": [
    "without preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b89217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.728\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88         9\n",
      "           1       0.75      0.54      0.63        94\n",
      "           2       0.65      0.93      0.77       138\n",
      "           3       0.69      0.77      0.73        65\n",
      "           4       0.74      0.62      0.67        81\n",
      "           5       0.88      0.68      0.77       113\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.78      0.72      0.74       500\n",
      "weighted avg       0.75      0.73      0.72       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "model1= Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "model1.fit(texts,labels)\n",
    "y_pred = model1.predict(test_texts)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e5f3d",
   "metadata": {},
   "source": [
    "with lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38ee1c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.728\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88         9\n",
      "           1       0.77      0.52      0.62        94\n",
      "           2       0.67      0.93      0.78       138\n",
      "           3       0.64      0.83      0.72        65\n",
      "           4       0.73      0.64      0.68        81\n",
      "           5       0.90      0.65      0.75       113\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.78      0.73      0.74       500\n",
      "weighted avg       0.75      0.73      0.72       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2= Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "model2.fit(lemme_text,labels)\n",
    "y_pred = model2.predict(test_lemme_text)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4320c2",
   "metadata": {},
   "source": [
    "with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9c3b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.734\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88         9\n",
      "           1       0.72      0.50      0.59        94\n",
      "           2       0.71      0.94      0.81       138\n",
      "           3       0.59      0.85      0.69        65\n",
      "           4       0.75      0.69      0.72        81\n",
      "           5       0.95      0.64      0.76       113\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.79      0.73      0.74       500\n",
      "weighted avg       0.76      0.73      0.73       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3= Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "model3.fit(stemm_text,labels)\n",
    "y_pred = model3.predict(test_stemm_text)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94376d80",
   "metadata": {},
   "source": [
    "accuracy is maximum for stemming so we will use stemming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "659e2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=stemm_text\n",
    "y_train= labels\n",
    "X_test=test_stemm_text\n",
    "y_test=test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9333c7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how did serfdom develop in and then leav russia '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "X_train=[preprocess(x) for x in X_train]\n",
    "\n",
    "X_test=[preprocess(x) for x in X_test]\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed064383",
   "metadata": {},
   "source": [
    "ii) Tf-idf sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901fe1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7998fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0832b75",
   "metadata": {},
   "source": [
    "ii) CBow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ead3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf7adc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v_model = api.load('word2vec-google-news-300') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abe3bb",
   "metadata": {},
   "source": [
    "cbow mean embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c10ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03427124,  0.01582718,  0.07673645,  0.18412781, -0.00122547,\n",
       "        0.02038574,  0.02956629, -0.13592529,  0.01013184,  0.05047607,\n",
       "       -0.09200287, -0.13833618, -0.07865906,  0.04612732, -0.20117188,\n",
       "        0.05702209,  0.03904724,  0.1227417 ,  0.09185791, -0.0657959 ,\n",
       "        0.03601074,  0.02427673,  0.06387329, -0.02804947,  0.0223403 ,\n",
       "        0.09883499, -0.07717705,  0.03207397, -0.0174942 ,  0.05000305,\n",
       "       -0.04743958,  0.00378418, -0.06468201, -0.08119202, -0.12365723,\n",
       "        0.09744263,  0.02725983, -0.03878784,  0.09840393,  0.06979752,\n",
       "        0.05728912,  0.02076721,  0.24707031, -0.04380798,  0.08026123,\n",
       "        0.0002594 ,  0.03794479, -0.02432251, -0.07565308,  0.07025146,\n",
       "       -0.02229309,  0.08152771,  0.00518417, -0.00271606, -0.04527283,\n",
       "        0.02480984, -0.13931274, -0.0615921 ,  0.07017899, -0.0953064 ,\n",
       "        0.04006958,  0.08468628, -0.09032249, -0.10282898, -0.09122467,\n",
       "       -0.0683136 ,  0.0397644 ,  0.04391479,  0.02353668,  0.11985779,\n",
       "        0.10058594,  0.10154343,  0.03504944, -0.00337219, -0.09418774,\n",
       "       -0.1481781 ,  0.16085815,  0.16670227, -0.02310181,  0.07485962,\n",
       "       -0.048172  , -0.00090027, -0.00996399,  0.03394699, -0.01229858,\n",
       "        0.02488232, -0.06958771,  0.07345581, -0.00643921,  0.10988235,\n",
       "       -0.0483551 ,  0.02781677, -0.13015747, -0.079422  ,  0.00561523,\n",
       "       -0.0687561 ,  0.01622009, -0.06951141,  0.09467316,  0.00763702,\n",
       "       -0.05555344, -0.04508209, -0.0406723 ,  0.12488937,  0.00934601,\n",
       "        0.01412964, -0.14286804,  0.02288818,  0.03275299, -0.0051918 ,\n",
       "       -0.12683105,  0.00033951, -0.02162266, -0.07187653,  0.01675415,\n",
       "        0.00263214,  0.03375244, -0.03036499,  0.03861237,  0.04278564,\n",
       "       -0.14505386,  0.00363159, -0.12698364,  0.06908798,  0.03610229,\n",
       "       -0.02450562, -0.05966759,  0.04081726, -0.03445435, -0.00793457,\n",
       "       -0.05724716,  0.01605225, -0.04668427,  0.02642822, -0.05818176,\n",
       "       -0.02937317, -0.06206894, -0.01208115,  0.00237274,  0.1086731 ,\n",
       "        0.00717735, -0.08416748,  0.00410461,  0.08753967,  0.06584167,\n",
       "        0.02088356, -0.03999329, -0.14315796, -0.03448486, -0.04202271,\n",
       "        0.17271423, -0.02770615, -0.01208496,  0.04524231,  0.00529003,\n",
       "        0.04150391, -0.05358171,  0.06388855,  0.00961304,  0.03846741,\n",
       "        0.04165649,  0.08079529, -0.03604507, -0.03661919,  0.06071091,\n",
       "       -0.13092041,  0.05779076, -0.02021408, -0.06473541,  0.01934814,\n",
       "       -0.07587376,  0.01348877, -0.02603149,  0.02650452,  0.06671143,\n",
       "       -0.02292633,  0.12924194, -0.15283203,  0.07369232, -0.02882576,\n",
       "       -0.06568909, -0.06213188, -0.03140259, -0.01927185,  0.03717041,\n",
       "        0.05252075,  0.00078869,  0.08309937,  0.06186104,  0.03166962,\n",
       "        0.06754303,  0.04023552, -0.04232025, -0.00783539,  0.04881287,\n",
       "       -0.00234985, -0.00749207, -0.0226593 ,  0.01411438, -0.15460205,\n",
       "       -0.00634766,  0.03625488,  0.05395508, -0.03392029, -0.04226685,\n",
       "       -0.05145264, -0.04373741,  0.00513458, -0.069767  , -0.04430389,\n",
       "       -0.04225922,  0.0374527 , -0.03410339, -0.0255394 , -0.12980652,\n",
       "       -0.01864624,  0.0692749 ,  0.01606369,  0.01617432, -0.03083801,\n",
       "       -0.05267334, -0.01081848,  0.09050751,  0.00924683,  0.03181076,\n",
       "       -0.09249878,  0.15048981,  0.0241127 , -0.01245117, -0.00086212,\n",
       "        0.14874268, -0.12259674,  0.04138184, -0.06425476,  0.02308655,\n",
       "       -0.05056   , -0.0284729 , -0.01742172,  0.13705444, -0.04963684,\n",
       "       -0.02246094, -0.02509689,  0.02409363, -0.04096079, -0.02687836,\n",
       "        0.01509857,  0.01599121,  0.08930206, -0.03474808,  0.01977539,\n",
       "        0.06846619,  0.05096245,  0.14712524,  0.0294342 , -0.02529526,\n",
       "       -0.05253601,  0.03788653, -0.00487518, -0.03201294, -0.09945679,\n",
       "        0.03045273, -0.00224304,  0.01731873,  0.05067444,  0.05566406,\n",
       "        0.08442688, -0.06195068, -0.01885033, -0.03244543,  0.00964355,\n",
       "       -0.0340271 ,  0.08444214,  0.12757874,  0.03860474,  0.04195684,\n",
       "       -0.03749084, -0.00500488, -0.03572083, -0.12576294,  0.07052422,\n",
       "       -0.00757599,  0.01654053,  0.05457306,  0.07107127, -0.02243614,\n",
       "       -0.02458191, -0.09085083,  0.02803898,  0.05987549,  0.03691101,\n",
       "       -0.15597534,  0.0965271 , -0.0942688 ,  0.04945374, -0.05839539,\n",
       "       -0.0125885 , -0.0246582 , -0.00523996,  0.02373505,  0.00791931])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_cbow(texts, model):\n",
    "    vectors = []\n",
    "    for sentence in texts:\n",
    "        words = sentence.split()\n",
    "        valid_words = [model[word] for word in words if word in model]\n",
    "        if valid_words:\n",
    "            vectors.append(sum(valid_words) / len(valid_words))\n",
    "        else:\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train_cbow_mean= vectorize_cbow(X_train, w2v_model)\n",
    "X_test_cbow_mean = vectorize_cbow(X_test, w2v_model)\n",
    "X_train_cbow_mean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac34bb",
   "metadata": {},
   "source": [
    "glove mean embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d44aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove = load_glove_embeddings(\"archive/glove.6B.100d.txt\")\n",
    "\n",
    "def sentence_to_vector(sentence, embeddings, dim=100):\n",
    "    words = preprocess(sentence)\n",
    "    vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "X_train_glove_mean = np.array([sentence_to_vector(sent, glove, 100) for sent in X_train])\n",
    "X_test_glove_mean= np.array([sentence_to_vector(sent, glove, 100) for sent in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ad1b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.7442964e-01,  3.6535317e-01,  1.5439278e-01, -2.0138146e-01,\n",
       "       -3.2791090e-01,  1.3251089e-01,  2.0701523e-01,  2.1957679e-01,\n",
       "       -4.5216241e-01,  1.4855900e-01,  6.1905539e-01, -5.6374019e-01,\n",
       "       -6.0469246e-01,  4.2445922e-01, -8.9672439e-02, -1.9194417e-01,\n",
       "       -4.4239253e-02, -7.3100992e-02, -8.1686571e-02,  1.5874349e-01,\n",
       "        5.9251869e-01,  9.1681108e-02,  1.6831284e-02,  4.7428277e-01,\n",
       "        1.1391300e-01,  5.1658505e-01,  2.1843876e-01,  1.4803204e-01,\n",
       "        2.0713256e-01, -2.2506511e-01,  3.3820912e-01,  7.2814637e-01,\n",
       "        1.2401687e-01,  2.7697900e-01,  4.2861646e-01,  1.6064940e-01,\n",
       "        4.8349366e-01,  2.2341575e-01,  2.9683730e-01,  3.6747631e-01,\n",
       "        2.0922235e-01, -5.9616637e-01, -9.4286492e-03, -5.7932597e-01,\n",
       "       -1.2543838e-01,  1.8999942e-01, -5.6021369e-01, -4.0297419e-02,\n",
       "        1.7896916e-01, -2.8541920e-01,  3.1735923e-02,  1.3076967e-01,\n",
       "        4.0279233e-01,  5.7824086e-03, -5.3424752e-01, -1.8206637e+00,\n",
       "        1.0690219e-03,  2.4061653e-01,  1.2175198e+00,  2.7438077e-01,\n",
       "       -3.9438763e-01, -1.2151713e-01, -6.1461568e-01, -4.0189922e-01,\n",
       "        7.9642147e-01,  4.8133585e-01,  5.6828386e-01,  6.3605005e-01,\n",
       "        3.3549371e-01,  3.0338758e-01, -1.2047875e-01,  1.8911195e-01,\n",
       "       -1.8657668e-01, -4.9180788e-01,  6.0811538e-01, -2.4284390e-01,\n",
       "        1.5889466e-01, -2.1790318e-01, -6.0767430e-01, -3.5985190e-01,\n",
       "        1.0194164e-01, -8.6648560e-01, -3.6909717e-01, -5.2376258e-01,\n",
       "       -7.0301223e-01,  1.9695100e-01, -1.1980026e-01, -9.0457278e-01,\n",
       "        5.5275481e-02, -1.3461532e-01, -6.0953832e-01,  4.4071817e-01,\n",
       "        4.2907569e-01,  6.1861545e-01, -3.4935515e-02, -3.0126205e-02,\n",
       "       -8.5927993e-03, -5.3489894e-01,  2.2431037e-01, -2.1060212e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_glove_mean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df523e1",
   "metadata": {},
   "source": [
    "skip gram mean embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45e1b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb02d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(tokens, model, dim=100):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "X_train_sg = np.array([sentence_to_vector(sent, w2v_model, 100) for sent in X_train])\n",
    "X_test_sg = np.array([sentence_to_vector(sent, w2v_model, 100) for sent in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c790a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12048084,  0.07750017,  0.03726451,  0.10009532,  0.08369804,\n",
       "       -0.04920781,  0.12045842,  0.11197897, -0.10836435, -0.09109332,\n",
       "        0.12698498, -0.11640978, -0.02451688, -0.02767149,  0.02473996,\n",
       "       -0.05596323,  0.04146416,  0.08166689, -0.09080675, -0.09528011,\n",
       "       -0.02630907, -0.02363455,  0.18667865,  0.01648281,  0.0512659 ,\n",
       "       -0.01690175, -0.0036977 ,  0.05939044, -0.07466883,  0.01759764,\n",
       "        0.02484157, -0.06096856,  0.1270047 ,  0.07521374, -0.05467458,\n",
       "        0.07259584, -0.01255775,  0.01153789,  0.04261676, -0.00975642,\n",
       "        0.11343464, -0.02531392, -0.187321  ,  0.13309714, -0.00458413,\n",
       "        0.11342674, -0.02931422, -0.03044664, -0.00760655,  0.05871193,\n",
       "        0.03507536, -0.1001359 , -0.00273306, -0.04859355, -0.06685618,\n",
       "       -0.0614187 ,  0.04211865, -0.07638345,  0.02386894,  0.04801695,\n",
       "       -0.06498439,  0.03779931,  0.09383649, -0.13053836, -0.1055854 ,\n",
       "        0.07172894,  0.00188703,  0.12946768, -0.0381165 , -0.04194868,\n",
       "        0.06120693,  0.12495729,  0.11053815,  0.1164974 , -0.005424  ,\n",
       "        0.05320962,  0.08421161, -0.02105791, -0.01481425, -0.10665005,\n",
       "       -0.07891438, -0.0049063 ,  0.06774592,  0.01141059, -0.02203917,\n",
       "       -0.06685191,  0.09136631, -0.18520725,  0.02295434, -0.06435753,\n",
       "        0.04843126,  0.0779352 ,  0.01151726,  0.03043188,  0.15933125,\n",
       "        0.04016599,  0.11848526, -0.09427013,  0.06422783, -0.0173947 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f1a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f0c53",
   "metadata": {},
   "source": [
    "now will first create word embeddigs using cbow and then combine using lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d43b6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def vectorize_cbow_sequence(texts, model, max_len):\n",
    "    sequences = []\n",
    "    for sentence in texts:\n",
    "        words = sentence.split()\n",
    "        word_vectors = [model[word] for word in words if word in model]\n",
    "        sequences.append(word_vectors)\n",
    "    \n",
    "    # Pad sequences with zeros (vectors of size model.vector_size)\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            # pad with zero vectors\n",
    "            padding = [np.zeros(model.vector_size)] * (max_len - len(seq))\n",
    "            seq.extend(padding)\n",
    "        else:\n",
    "            seq = seq[:max_len]  # truncate\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2004deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30  # or use max(len(s.split()) for s in X_train)\n",
    "X_train_cbow= vectorize_cbow_sequence(X_train, w2v_model, max_len)\n",
    "X_test_cbow = vectorize_cbow_sequence(X_test, w2v_model, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "924bc201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5452"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19403ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM\n",
    "\n",
    "max_len = 30           # number of words in a sentence\n",
    "embedding_dim = 300   # GloVe or other embeddings\n",
    "lstm_units = 128       # output dimension\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(max_len, embedding_dim))\n",
    "\n",
    "# LSTM layer — outputs final hidden state\n",
    "x = LSTM(lstm_units, return_sequences=False)(inputs)\n",
    "\n",
    "# Create model\n",
    "model0 = Model(inputs, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "786275b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 30, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3f95968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train_cbow_word_combined=model0.predict(X_train_cbow)\n",
    "X_test_cbow_word_combined=model0.predict(X_test_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eceb649b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 128)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cbow_word_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09017a",
   "metadata": {},
   "source": [
    "now will first create word embeddigs using glove and then combine using lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b07f1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # number of words in a sentence\n",
    "embedding_dim1 = 100  # GloVe or other embeddings\n",
    "     # output dimension\n",
    "\n",
    "# Input layer\n",
    "inputs1 = Input(shape=(max_len, embedding_dim1))\n",
    "\n",
    "# LSTM layer — outputs final hidden state\n",
    "x1= LSTM(lstm_units, return_sequences=False)(inputs1)\n",
    "\n",
    "# Create model\n",
    "model01 = Model(inputs1, x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f36f871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentence_to_glove_vectors(texts, glove, max_len, embedding_dim=100):\n",
    "    all_padded_sequences = []\n",
    "\n",
    "    for sentence in texts:\n",
    "        words = sentence.lower().split()\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            vector = glove.get(word)\n",
    "            if vector is not None:\n",
    "                vectors.append(vector)\n",
    "            else:\n",
    "                vectors.append(np.zeros(embedding_dim))\n",
    "        \n",
    "        # Pad or truncate each sentence to max_len\n",
    "        if len(vectors) < max_len:\n",
    "            padding = [np.zeros(embedding_dim)] * (max_len - len(vectors))\n",
    "            vectors.extend(padding)\n",
    "        else:\n",
    "            vectors = vectors[:max_len]\n",
    "\n",
    "        all_padded_sequences.append(vectors)\n",
    "    \n",
    "    return np.array(all_padded_sequences)  # shape: (num_sentences, max_len, embedding_dim)\n",
    "\n",
    "    \n",
    "X_train_glove=sentence_to_glove_vectors(X_train,glove,30) \n",
    "X_test_glove=sentence_to_glove_vectors(X_test,glove,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fe28729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train_glove_combined=model01.predict(X_train_glove)\n",
    "X_test_glove_combined=model01.predict(X_test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05d2ab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 128)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_glove_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755c9da",
   "metadata": {},
   "source": [
    "now will use bidirectional lstm to combine \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175c386",
   "metadata": {},
   "source": [
    "1) will combine cbow word vetors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a06764c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Bidirectional,LSTM\n",
    "\n",
    "max_len = 30           # number of words in a sentence\n",
    "embedding_dim = 300   # GloVe or other embeddings\n",
    "lstm_units = 128       # output dimension\n",
    "\n",
    "# Input layer\n",
    "inputs11 = Input(shape=(max_len, embedding_dim))\n",
    "\n",
    "# LSTM layer — outputs final hidden state\n",
    "x11= Bidirectional(LSTM(lstm_units, return_sequences=False))(inputs11)\n",
    "\n",
    "# Create model\n",
    "model11 = Model(inputs11, x11)\n",
    "X_train_cbow_combined_bilstm= model11.predict(X_train_cbow)\n",
    "X_test_cbow_combined_bilstm= model11.predict(X_test_cbow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16028693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 256)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cbow_combined_bilstm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8b4e4",
   "metadata": {},
   "source": [
    "2. will combine glove word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2221b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    }
   ],
   "source": [
    "max_len = 30           # number of words in a sentence\n",
    "embedding_dim = 100  # GloVe or other embeddings\n",
    "lstm_units = 128       # output dimension\n",
    "\n",
    "# Input layer\n",
    "inputs12 = Input(shape=(max_len, embedding_dim))\n",
    "\n",
    "# LSTM layer — outputs final hidden state\n",
    "x12= Bidirectional(LSTM(lstm_units, return_sequences=False))(inputs12)\n",
    "\n",
    "# Create model\n",
    "model12 = Model(inputs12, x12)\n",
    "X_train_glove_combined_bilstm= model12.predict(X_train_glove)\n",
    "X_test_glove_combined_bilstm= model12.predict(X_test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a81b8c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 256)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_glove_combined_bilstm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd51a49",
   "metadata": {},
   "source": [
    "5) using ML classifier model on cbow and glove  word embedding \n",
    "\n",
    "i) LSTM + Neural networrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c88d9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">219,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m219,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">228,294</span> (891.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m228,294\u001b[0m (891.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">228,294</span> (891.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m228,294\u001b[0m (891.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_len, w2v_model.vector_size), return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax')) \n",
    "  # for multiclass classification\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a3d66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_lstm = to_categorical(y_train, 6)\n",
    "y_test_lstm = to_categorical(y_test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fec2e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 30, 300)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96d83b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.2708 - loss: 1.6252 - val_accuracy: 0.5040 - val_loss: 1.3155\n",
      "Epoch 2/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.5548 - loss: 1.1642 - val_accuracy: 0.6880 - val_loss: 0.9270\n",
      "Epoch 3/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.6932 - loss: 0.8839 - val_accuracy: 0.7720 - val_loss: 0.7159\n",
      "Epoch 4/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7283 - loss: 0.7757 - val_accuracy: 0.7880 - val_loss: 0.6677\n",
      "Epoch 5/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7799 - loss: 0.6777 - val_accuracy: 0.7720 - val_loss: 0.6236\n",
      "Epoch 6/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7932 - loss: 0.6051 - val_accuracy: 0.7980 - val_loss: 0.6067\n",
      "Epoch 7/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.8029 - loss: 0.5791 - val_accuracy: 0.7900 - val_loss: 0.6025\n",
      "Epoch 8/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8138 - loss: 0.5610 - val_accuracy: 0.7980 - val_loss: 0.6543\n",
      "Epoch 9/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.8186 - loss: 0.5509 - val_accuracy: 0.8040 - val_loss: 0.5718\n",
      "Epoch 10/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.8280 - loss: 0.5104 - val_accuracy: 0.8160 - val_loss: 0.5706\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train_cbow,y_train_lstm,validation_data=(X_test_cbow,y_test_lstm),epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52fe76",
   "metadata": {},
   "source": [
    "cbow_ACCURACY=84% cbow_val_ACCURACY =82%\n",
    "\n",
    "now on to glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12e976d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 30, 100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43e3d48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m117,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,894</span> (491.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m125,894\u001b[0m (491.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,894</span> (491.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m125,894\u001b[0m (491.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(128, input_shape=(max_len,100), return_sequences=False))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(6, activation='softmax'))  # for multiclass classification\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2dd93f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.2889 - loss: 1.6260 - val_accuracy: 0.5060 - val_loss: 1.1753\n",
      "Epoch 2/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.4594 - loss: 1.2958 - val_accuracy: 0.5060 - val_loss: 1.1108\n",
      "Epoch 3/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.5237 - loss: 1.1543 - val_accuracy: 0.6180 - val_loss: 0.9614\n",
      "Epoch 4/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.6471 - loss: 0.9417 - val_accuracy: 0.7540 - val_loss: 0.7891\n",
      "Epoch 5/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7193 - loss: 0.8044 - val_accuracy: 0.7880 - val_loss: 0.6548\n",
      "Epoch 6/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.7511 - loss: 0.6917 - val_accuracy: 0.8160 - val_loss: 0.5220\n",
      "Epoch 7/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.7784 - loss: 0.6332 - val_accuracy: 0.7900 - val_loss: 0.6203\n",
      "Epoch 8/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.7875 - loss: 0.6119 - val_accuracy: 0.8080 - val_loss: 0.5573\n",
      "Epoch 9/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8021 - loss: 0.5636 - val_accuracy: 0.7920 - val_loss: 0.5871\n",
      "Epoch 10/10\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.8158 - loss: 0.5341 - val_accuracy: 0.8400 - val_loss: 0.4813\n"
     ]
    }
   ],
   "source": [
    "history2=model2.fit(X_train_glove,y_train_lstm,validation_data=(X_test_glove,y_test_lstm),epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0055f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n"
     ]
    }
   ],
   "source": [
    "arr=model2.predict(np.array([X_train_glove[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d125d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = np.argmax(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e38268",
   "metadata": {},
   "source": [
    " glove_accuracy= 83% glove_val_accuracy = 84%\n",
    "\n",
    " ii) logistic regression\n",
    " cbow +lstm+logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9dfd6945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cbow_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "022a4517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.194\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.19      1.00      0.32        94\n",
      "           2       0.00      0.00      0.00       138\n",
      "           3       0.75      0.05      0.09        65\n",
      "           4       0.00      0.00      0.00        81\n",
      "           5       0.00      0.00      0.00       113\n",
      "\n",
      "    accuracy                           0.19       500\n",
      "   macro avg       0.16      0.17      0.07       500\n",
      "weighted avg       0.13      0.19      0.07       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_cbow_word_combined,labels)\n",
    "y_pred = clf.predict(X_test_cbow_word_combined)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888e277",
   "metadata": {},
   "source": [
    "glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60fc35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.372\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.26      0.48      0.34        94\n",
      "           2       0.42      0.96      0.59       138\n",
      "           3       0.53      0.12      0.20        65\n",
      "           4       0.00      0.00      0.00        81\n",
      "           5       0.00      0.00      0.00       113\n",
      "\n",
      "    accuracy                           0.37       500\n",
      "   macro avg       0.20      0.26      0.19       500\n",
      "weighted avg       0.24      0.37      0.25       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "clf2 = LogisticRegression(max_iter=1000)\n",
    "clf2.fit(X_train_glove_combined,labels)\n",
    "y_pred = clf2.predict(X_test_glove_combined)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59a086",
   "metadata": {},
   "source": [
    "cbow+mean+logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "faca5a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.752\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.62         9\n",
      "           1       0.65      0.57      0.61        94\n",
      "           2       0.69      0.86      0.76       138\n",
      "           3       0.88      0.88      0.88        65\n",
      "           4       0.73      0.79      0.76        81\n",
      "           5       0.92      0.69      0.79       113\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.76      0.72      0.74       500\n",
      "weighted avg       0.76      0.75      0.75       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf3 = LogisticRegression(max_iter=1000)\n",
    "clf3.fit(X_train_cbow_mean,labels)\n",
    "y_pred = clf3.predict(X_test_cbow_mean)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365f870",
   "metadata": {},
   "source": [
    "glove+mean+logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7aae87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 100)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_glove_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d27d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.392\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.31      0.37      0.34        94\n",
      "           2       0.53      0.62      0.58       138\n",
      "           3       0.29      0.48      0.36        65\n",
      "           4       0.35      0.36      0.36        81\n",
      "           5       0.39      0.13      0.20       113\n",
      "\n",
      "    accuracy                           0.39       500\n",
      "   macro avg       0.31      0.33      0.31       500\n",
      "weighted avg       0.39      0.39      0.37       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rohan\\OneDrive\\ドキュメント\\Desktop\\frame to fables\\frame_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "clf4 = LogisticRegression(max_iter=1000)\n",
    "clf4.fit(X_train_glove_mean,labels)\n",
    "y_pred = clf4.predict(X_test_glove_mean)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4056fdb",
   "metadata": {},
   "source": [
    "ii) decision trees \n",
    "cbow+mean+decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "920bdac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a3082c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=20, random_state=42) \n",
    "clf.fit(X_train_cbow_mean, labels)\n",
    "\n",
    "# Step 4: Predict and evaluate\n",
    "y_pred = clf.predict(X_test_cbow_mean)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(labels, y_pred))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4664471b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5452"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7644653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.468\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.44      0.32         9\n",
      "           1       0.35      0.38      0.37        94\n",
      "           2       0.57      0.57      0.57       138\n",
      "           3       0.42      0.54      0.47        65\n",
      "           4       0.45      0.49      0.47        81\n",
      "           5       0.56      0.36      0.44       113\n",
      "\n",
      "    accuracy                           0.47       500\n",
      "   macro avg       0.43      0.46      0.44       500\n",
      "weighted avg       0.48      0.47      0.47       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdaf0b",
   "metadata": {},
   "source": [
    "ii) 2 glove+mean+decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba7df311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.44      0.47         9\n",
      "           1       0.25      0.28      0.26        94\n",
      "           2       0.47      0.49      0.48       138\n",
      "           3       0.21      0.31      0.25        65\n",
      "           4       0.28      0.28      0.28        81\n",
      "           5       0.36      0.21      0.27       113\n",
      "\n",
      "    accuracy                           0.33       500\n",
      "   macro avg       0.35      0.34      0.34       500\n",
      "weighted avg       0.34      0.33      0.33       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=20, random_state=42) \n",
    "clf.fit(X_train_glove_mean, labels)\n",
    "\n",
    "# Step 4: Predict and evaluate\n",
    "y_pred = clf.predict(X_test_glove_mean)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1628bb3",
   "metadata": {},
   "source": [
    "### the best accuracy comes with STEMMING + CBOW + LSTM  which is 84% on training data and 82 on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6fc11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frame_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
