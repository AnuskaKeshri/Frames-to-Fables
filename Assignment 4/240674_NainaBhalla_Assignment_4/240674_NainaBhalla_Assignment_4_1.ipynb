{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05373b1b-ef95-449c-89c7-40069b5588ea",
   "metadata": {},
   "source": [
    "# Assignment 4: Text Classification on TREC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e",
   "metadata": {},
   "source": [
    "We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n",
    "\n",
    "This is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment! \n",
    "\n",
    "Your experimentation should be 4-tiered-\n",
    "\n",
    "i) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n",
    "\n",
    "ii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n",
    "\n",
    "iii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n",
    "\n",
    "iv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n",
    "\n",
    "So applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n",
    "\n",
    "NOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9",
   "metadata": {},
   "source": [
    "### Helper Code to get you started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d08592-c633-4764-a60a-4937fd768cb4",
   "metadata": {},
   "source": [
    "I have added some helper code to show you how to load the TERC dataset and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91cbe1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: How did serfdom develop in and then leave Russia ?\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\", trust_remote_code=True)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(\"Sample Question:\", train_data[0]['text'])\n",
    "print(\"Label:\", train_data[0]['coarse_label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b33d7",
   "metadata": {},
   "source": [
    "## Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e52fb4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\naina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\naina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\naina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba44e4",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "918276c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= train_data.to_pandas()\n",
    "test_df = test_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3058fd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>coarse_label</th>\n",
       "      <th>fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  coarse_label  fine_label\n",
       "0  How did serfdom develop in and then leave Russ...             2          26\n",
       "1   What films featured the character Popeye Doyle ?             1           5\n",
       "2  How can I find a list of celebrities ' real na...             2          26\n",
       "3  What fowl grabs the spotlight after the Chines...             1           2\n",
       "4                    What is the full form of .com ?             0           1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5b1bf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>coarse_label</th>\n",
       "      <th>fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How far is it from Denver to Aspen ?</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What county is Modesto , California in ?</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who was Galileo ?</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is an atom ?</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did Hawaii become a state ?</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text  coarse_label  fine_label\n",
       "0      How far is it from Denver to Aspen ?             5          40\n",
       "1  What county is Modesto , California in ?             4          32\n",
       "2                         Who was Galileo ?             3          31\n",
       "3                         What is an atom ?             2          24\n",
       "4          When did Hawaii become a state ?             5          39"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6ec6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['fine_label'], inplace=True)\n",
    "test_df.drop(columns=['fine_label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cdcaef9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>coarse_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  coarse_label\n",
       "0  How did serfdom develop in and then leave Russ...             2\n",
       "1   What films featured the character Popeye Doyle ?             1\n",
       "2  How can I find a list of celebrities ' real na...             2\n",
       "3  What fowl grabs the spotlight after the Chines...             1\n",
       "4                    What is the full form of .com ?             0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5fa4cae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coarse_label\n",
       "1    1250\n",
       "3    1223\n",
       "2    1162\n",
       "5     896\n",
       "4     835\n",
       "0      86\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['coarse_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3ede5",
   "metadata": {},
   "source": [
    "## Functions and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfba365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b0b70",
   "metadata": {},
   "source": [
    "**Preprocessing function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4330f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, method='none'):\n",
    "    #Converting to lowercase\n",
    "    text = text.lower()\n",
    "    #Removing punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    # Stemming or Lemmatization according to the method specified\n",
    "    if method == 'stem':\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    elif method == 'lemma':\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    else:\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1d894",
   "metadata": {},
   "source": [
    "**Creating different columns for stemmed and lemmatized text** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe6fda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['lemmatized_text'] = train_df['text'].apply(lambda x: preprocess(x, method='lemma'))\n",
    "test_df['lemmatized_text'] = test_df['text'].apply(lambda x: preprocess(x, method='lemma'))\n",
    "train_df['stemmed_text'] = train_df['text'].apply(lambda x: preprocess(x, method='stem'))\n",
    "test_df['stemmed_text'] = test_df['text'].apply(lambda x: preprocess(x, method='stem'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db1f23",
   "metadata": {},
   "source": [
    "**Loading different pre-trained embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = api.load(\"glove-wiki-gigaword-100\")\n",
    "cbow_embeddings = api.load(\"word2vec-google-news-300\")\n",
    "fasttext_embeddings = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6accc",
   "metadata": {},
   "source": [
    "**Functions to vectorize and embed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ebcf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed text using only the mean of the word embedding vectors\n",
    "def embed_text_with_model(corpus, embeddings,type='mean'):\n",
    "    embedded = []\n",
    "    for sentence in tqdm(corpus):\n",
    "        words = sentence.split()\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        if vectors:\n",
    "            if type == 'mean':\n",
    "                embedded.append(np.mean(vectors, axis=0))\n",
    "            elif type == 'max_pooling':\n",
    "                embedded.append(np.max(vectors, axis=0))\n",
    "            \n",
    "        else:\n",
    "            embedded.append(np.zeros(embeddings.vector_size))\n",
    "    return np.array(embedded)\n",
    "\n",
    "# Vectorization \n",
    "def vectorize(method, train, test,type='mean'):\n",
    "    if method == 'BoW':\n",
    "        vectorizer = CountVectorizer()\n",
    "        return vectorizer.fit_transform(train), vectorizer.transform(test)\n",
    "    elif method == 'TF-IDF':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        return vectorizer.fit_transform(train), vectorizer.transform(test)\n",
    "    elif method == 'CBoW':\n",
    "        embeddings = cbow_embeddings\n",
    "        return embed_text_with_model(train, embeddings,type), embed_text_with_model(test, embeddings,type)\n",
    "    elif method == 'GloVe':\n",
    "        embeddings = glove_embeddings\n",
    "        return embed_text_with_model(train, embeddings,type), embed_text_with_model(test, embeddings,type)\n",
    "    elif method == 'FastText':\n",
    "        embeddings = fasttext_embeddings\n",
    "        return embed_text_with_model(train, embeddings,type), embed_text_with_model(test, embeddings,type)\n",
    "    else:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "17770e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= train_df['coarse_label']\n",
    "y_test = test_df['coarse_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f64a319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_methods = ['BoW', 'TF-IDF','CBoW', 'GloVe', 'FastText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dbce2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0d3862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'SVM': SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd9c0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_methods = ['lemma', 'stem','none']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce19cc9",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "018d17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing: lemma\n",
      "\n",
      "Preprocessing: stem\n",
      "\n",
      "Preprocessing: none\n"
     ]
    }
   ],
   "source": [
    "#Baseline model: Logistic Regression with BoW\n",
    "baseline_results = []\n",
    "\n",
    "for preprocessing in preprocessing_methods:\n",
    "    print(f\"\\nPreprocessing: {preprocessing}\")\n",
    "    if preprocessing == 'lemma':\n",
    "        train_texts = train_df['lemmatized_text']\n",
    "        test_texts = test_df['lemmatized_text']\n",
    "    elif preprocessing == 'stem':\n",
    "        train_texts = train_df['stemmed_text']\n",
    "        test_texts = test_df['stemmed_text']\n",
    "    else:\n",
    "        train_texts = train_df['text']\n",
    "        test_texts = test_df['text']\n",
    "\n",
    "    X_train, X_test = vectorize('BoW', train_texts, test_texts)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    acc = evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    baseline_results.append({\n",
    "        'Preprocessing': preprocessing,\n",
    "        'Vectorizer': 'BoW',\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Accuracy': acc,\n",
    "        'Combination': \"Mean\"\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d052d97",
   "metadata": {},
   "source": [
    "**Results of baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0c901df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Tests:\n",
      "  Preprocessing Vectorizer                Model  Accuracy Combination\n",
      "0          none        BoW  Logistic Regression     0.852        Mean\n",
      "1         lemma        BoW  Logistic Regression     0.756        Mean\n",
      "2          stem        BoW  Logistic Regression     0.754        Mean\n"
     ]
    }
   ],
   "source": [
    "baseline = pd.DataFrame(baseline_results)\n",
    "print(\"\\nBaseline Tests:\")\n",
    "print(baseline.sort_values(by='Accuracy', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114e09f",
   "metadata": {},
   "source": [
    "### Since NO PREPROCESSING works the best, continuing with it going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf76fe",
   "metadata": {},
   "source": [
    "**Training and Evaluating models with MAX POOLING combining and no preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "85d9ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorization: BoW\n",
      "\n",
      "Model: Logistic Regression\n",
      "\n",
      "Model: Decision Tree\n",
      "\n",
      "Model: Random Forest\n",
      "\n",
      "Model: SVM\n",
      "\n",
      "Vectorization: TF-IDF\n",
      "\n",
      "Model: Logistic Regression\n",
      "\n",
      "Model: Decision Tree\n",
      "\n",
      "Model: Random Forest\n",
      "\n",
      "Model: SVM\n",
      "\n",
      "Vectorization: CBoW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5452/5452 [00:00<00:00, 39018.58it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 40332.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Decision Tree\n",
      "\n",
      "Model: Random Forest\n",
      "\n",
      "Model: SVM\n",
      "\n",
      "Vectorization: GloVe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5452/5452 [00:00<00:00, 38703.57it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 39091.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Decision Tree\n",
      "\n",
      "Model: Random Forest\n",
      "\n",
      "Model: SVM\n",
      "\n",
      "Vectorization: FastText\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5452/5452 [00:00<00:00, 36009.98it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 44282.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n",
      "\n",
      "Model: Decision Tree\n",
      "\n",
      "Model: Random Forest\n",
      "\n",
      "Model: SVM\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for vec_method in vectorization_methods:\n",
    "    print(f\"\\nVectorization: {vec_method}\")\n",
    "    X_train, X_test = vectorize(vec_method, train_df['text'], test_df['text'],type='max_pooling')\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        acc = evaluate(model, X_train, y_train, X_test, y_test)\n",
    "        results.append({\n",
    "            'Preprocessing': 'None',\n",
    "            'Vectorizer': vec_method,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': acc,\n",
    "            'Combination': \"Max Pooling\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c141a54",
   "metadata": {},
   "source": [
    "### Using LSTM combining method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a690d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b990e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f217790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['text'].apply(lambda x: preprocess(x, method='none'))\n",
    "test_df['clean_text'] = test_df['text'].apply(lambda x: preprocess(x, method='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef05e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts, embeddings, dim):\n",
    "    sequences = []\n",
    "    for sentence in texts:\n",
    "        tokens = sentence.split()\n",
    "        vecs = [embeddings[w] if w in embeddings else np.zeros(dim) for w in tokens]\n",
    "        sequences.append(vecs)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1390d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {\n",
    "    'Word2Vec': cbow_embeddings,\n",
    "    'GloVe': glove_embeddings,\n",
    "    'FastText': fasttext_embeddings\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bbe5ec",
   "metadata": {},
   "source": [
    "**Converting Labels to Categorical for LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2a00020",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train, 6)\n",
    "y_test_cat = to_categorical(y_test, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3a3bc",
   "metadata": {},
   "source": [
    "**Defining different dimensions for different embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2345fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = {\n",
    "    'Word2Vec': 300,\n",
    "    'GloVe': 100,\n",
    "    'FastText': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48afea",
   "metadata": {},
   "source": [
    "**Training and evaluating models using different embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "07f59ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using embedding: Word2Vec\n",
      "Epoch 1/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.4188 - loss: 1.4766 - val_accuracy: 0.6856 - val_loss: 0.9532\n",
      "Epoch 2/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6973 - loss: 0.8767 - val_accuracy: 0.7021 - val_loss: 0.8266\n",
      "Epoch 3/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7565 - loss: 0.7152 - val_accuracy: 0.7214 - val_loss: 0.7744\n",
      "Epoch 4/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8065 - loss: 0.5784 - val_accuracy: 0.7479 - val_loss: 0.7578\n",
      "Epoch 5/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8235 - loss: 0.5272 - val_accuracy: 0.7479 - val_loss: 0.7573\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Using embedding: GloVe\n",
      "Epoch 1/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.4001 - loss: 1.4633 - val_accuracy: 0.5995 - val_loss: 1.0478\n",
      "Epoch 2/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.6322 - loss: 0.9726 - val_accuracy: 0.6774 - val_loss: 0.9104\n",
      "Epoch 3/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6974 - loss: 0.8037 - val_accuracy: 0.6948 - val_loss: 0.8550\n",
      "Epoch 4/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7270 - loss: 0.7484 - val_accuracy: 0.7113 - val_loss: 0.8494\n",
      "Epoch 5/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7861 - loss: 0.6126 - val_accuracy: 0.7241 - val_loss: 0.7970\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\n",
      "Using embedding: FastText\n",
      "Epoch 1/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.3512 - loss: 1.5738 - val_accuracy: 0.6544 - val_loss: 1.0736\n",
      "Epoch 2/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.6616 - loss: 1.0511 - val_accuracy: 0.6874 - val_loss: 0.9703\n",
      "Epoch 3/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7041 - loss: 0.8860 - val_accuracy: 0.7030 - val_loss: 0.8579\n",
      "Epoch 4/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7155 - loss: 0.8260 - val_accuracy: 0.7113 - val_loss: 0.8252\n",
      "Epoch 5/5\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7481 - loss: 0.7297 - val_accuracy: 0.7232 - val_loss: 0.8087\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n"
     ]
    }
   ],
   "source": [
    "for name, embedding in embeddings.items():\n",
    "    print(f\"\\nUsing embedding: {name}\")\n",
    "    dim = embedding_dims[name]\n",
    "    train_seqs = texts_to_sequences(train_df['clean_text'], embedding, dim=dim)\n",
    "    test_seqs = texts_to_sequences(test_df['clean_text'], embedding, dim=dim)\n",
    "\n",
    "    maxlen = max(max(len(s) for s in train_seqs), max(len(s) for s in test_seqs))\n",
    "    X_train_pad = pad_sequences(train_seqs, maxlen=maxlen, dtype='float32', padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(test_seqs, maxlen=maxlen, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "    input = Input(shape=(maxlen, dim))\n",
    "    x = Masking(mask_value=0.0)(input)\n",
    "    x = LSTM(128)(x) \n",
    "    output = Dense(6, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_pad, y_train_cat, epochs=5, batch_size=32, validation_split=0.2)\n",
    "    y_pred = model.predict(X_test_pad)\n",
    "    acc = accuracy_score(np.argmax(y_test_cat, axis=1), np.argmax(y_pred, axis=1))\n",
    "    results.append({\n",
    "            'Preprocessing': 'None',\n",
    "            'Vectorizer': name,\n",
    "            'Model': 'LSTM',\n",
    "            'Accuracy': acc,\n",
    "            'Combination': \" \"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270d27b",
   "metadata": {},
   "source": [
    "### Using Doc2Vec Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "df57833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess and tag\n",
    "train_tagged = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) \n",
    "                for i, doc in enumerate(train_df['clean_text'])]\n",
    "test_tagged = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) \n",
    "               for i, doc in enumerate(test_df['clean_text'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5d026f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(vector_size=100, window=5, min_count=2, workers=4, epochs=40)\n",
    "d2v_model.build_vocab(train_tagged)\n",
    "d2v_model.train(train_tagged, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14b87e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vectors(model, tagged_docs):\n",
    "    return np.array([model.infer_vector(doc.words) for doc in tagged_docs])\n",
    "\n",
    "X_train_d2v = get_doc_vectors(d2v_model, train_tagged)\n",
    "X_test_d2v = get_doc_vectors(d2v_model, test_tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4749998",
   "metadata": {},
   "source": [
    "**Models with Doc2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "11684f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'SVM': SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd41a76",
   "metadata": {},
   "source": [
    "**Training and evaluating models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de8f2ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n",
      "\n",
      "Model: Decision Tree\n",
      "\n",
      "Model: Random Forest\n",
      "\n",
      "Model: SVM\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        acc = evaluate(model, X_train_d2v, y_train, X_test_d2v, y_test)\n",
    "        results.append({\n",
    "            'Preprocessing': 'None',\n",
    "            'Vectorizer': vec_method,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': acc,\n",
    "            'Combination': \"Doc2Vec\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c21759",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8d8aac07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Table:\n",
      "   Preprocessing Vectorizer                Model  Accuracy  Combination\n",
      "0           None     TF-IDF                  SVM     0.864  Max Pooling\n",
      "1           None        BoW  Logistic Regression     0.852  Max Pooling\n",
      "2           None     TF-IDF  Logistic Regression     0.852  Max Pooling\n",
      "3           None        BoW        Random Forest     0.836  Max Pooling\n",
      "4           None        BoW                  SVM     0.836  Max Pooling\n",
      "5           None       CBoW                  SVM     0.834  Max Pooling\n",
      "6           None     TF-IDF        Random Forest     0.828  Max Pooling\n",
      "7           None        BoW        Decision Tree     0.826  Max Pooling\n",
      "8           None   FastText        Random Forest     0.800  Max Pooling\n",
      "9           None       CBoW        Random Forest     0.788  Max Pooling\n",
      "10          None   Word2Vec                 LSTM     0.770             \n",
      "11          None   FastText  Logistic Regression     0.762  Max Pooling\n",
      "12          None       CBoW  Logistic Regression     0.760  Max Pooling\n",
      "13          None      GloVe        Random Forest     0.752  Max Pooling\n",
      "14          None     TF-IDF        Decision Tree     0.752  Max Pooling\n",
      "15          None   FastText                  SVM     0.752  Max Pooling\n",
      "16          None      GloVe                 LSTM     0.748             \n",
      "17          None   FastText                 LSTM     0.708             \n",
      "18          None      GloVe                  SVM     0.672  Max Pooling\n",
      "19          None   FastText        Decision Tree     0.660  Max Pooling\n",
      "20          None      GloVe        Decision Tree     0.606  Max Pooling\n",
      "21          None      GloVe  Logistic Regression     0.602  Max Pooling\n",
      "22          None       CBoW        Decision Tree     0.586  Max Pooling\n",
      "23          None   FastText        Random Forest     0.584      Doc2Vec\n",
      "24          None   FastText                  SVM     0.524      Doc2Vec\n",
      "25          None   FastText        Decision Tree     0.412      Doc2Vec\n",
      "26          None   FastText  Logistic Regression     0.342      Doc2Vec\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary Table:\")\n",
    "print(results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd754e4",
   "metadata": {},
   "source": [
    "### Final Results:\n",
    "Best Performing Combination\n",
    "- Preprocessing: None\n",
    "- Vectorizer: TF-IDF\n",
    "- Model: SVM\n",
    "- Accuracy: 0.864\n",
    "- Combination: Max Pooling\n",
    "\n",
    "This portrays that simple vectorizers like TF-IDF with strong classical models like SVM, can outperform complex approaches such as LSTMs—especially when working with relatively small or moderate-sized datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
