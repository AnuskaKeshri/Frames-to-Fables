{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"05373b1b-ef95-449c-89c7-40069b5588ea","cell_type":"markdown","source":"# Assignment 4: Text Classification on TREC dataset","metadata":{"id":"05373b1b-ef95-449c-89c7-40069b5588ea"}},{"id":"1d2ad257-ec97-47bd-8cca-a2655fa5d92e","cell_type":"markdown","source":"We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n\nThis is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment!\n\nYour experimentation should be 4-tiered-\n\ni) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n\nii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n\niii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n\niv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n\nSo applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n\nNOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies.","metadata":{"id":"1d2ad257-ec97-47bd-8cca-a2655fa5d92e"}},{"id":"0cca5a12-6ddf-4895-a962-fd8fac4ad1f9","cell_type":"markdown","source":"### Helper Code to get you started","metadata":{"id":"0cca5a12-6ddf-4895-a962-fd8fac4ad1f9"}},{"id":"54d08592-c633-4764-a60a-4937fd768cb4","cell_type":"markdown","source":"I have added some helper code to show you how to load the TERC dataset and use it.","metadata":{"id":"54d08592-c633-4764-a60a-4937fd768cb4"}},{"id":"7b233cd7","cell_type":"code","source":"!pip install -q datasets nltk scikit-learn gensim fasttext gensim wget\nfrom datasets import load_dataset\nimport pandas as pd\nimport numpy as np\n#from sklearn.model_selection import train_test_split\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('omw-1.4')","metadata":{"id":"7b233cd7","outputId":"07ae7855-e541-49c6-c1b8-e02365986b08","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:42:28.021854Z","iopub.execute_input":"2025-06-16T17:42:28.022312Z","iopub.status.idle":"2025-06-16T17:42:43.315649Z","shell.execute_reply.started":"2025-06-16T17:42:28.022277Z","shell.execute_reply":"2025-06-16T17:42:43.314486Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":21},{"id":"d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1","cell_type":"code","source":"!pip install -q datasets\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"trec\", trust_remote_code=True)\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\nprint(\"Sample Question:\", train_data[0]['text'])\nprint(\"Label:\", train_data[0]['coarse_label'])\n","metadata":{"id":"d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1","outputId":"35ef5bb4-121d-4178-bd77-156a1eff6f23","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:42:43.317945Z","iopub.execute_input":"2025-06-16T17:42:43.318231Z","iopub.status.idle":"2025-06-16T17:42:48.431835Z","shell.execute_reply.started":"2025-06-16T17:42:43.318204Z","shell.execute_reply":"2025-06-16T17:42:48.430437Z"}},"outputs":[{"name":"stdout","text":"Sample Question: How did serfdom develop in and then leave Russia ?\nLabel: 2\n","output_type":"stream"}],"execution_count":22},{"id":"3bf43aa6","cell_type":"code","source":"train_texts = [sample['text'] for sample in train_data]\ntrain_labels = [sample['coarse_label'] for sample in train_data]\n\ntest_texts = [sample['text'] for sample in test_data]\ntest_labels = [sample['coarse_label'] for sample in test_data]","metadata":{"id":"3bf43aa6","outputId":"34d1b537-d20d-46aa-f6f7-f2a71e87ca05","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:42:48.433598Z","iopub.execute_input":"2025-06-16T17:42:48.433911Z","iopub.status.idle":"2025-06-16T17:42:48.854520Z","shell.execute_reply.started":"2025-06-16T17:42:48.433882Z","shell.execute_reply":"2025-06-16T17:42:48.853452Z"}},"outputs":[],"execution_count":23},{"id":"058ad1a1","cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport re\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\ndef preprocess(text, method='raw'):\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    tokens = word_tokenize(text.lower())\n    tokens = [word for word in tokens if word not in stop_words]\n\n    if method == 'lemmatize':\n        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    elif method == 'stem':\n        tokens = [stemmer.stem(word) for word in tokens]\n\n    return ' '.join(tokens)\n\n# Apply preprocessing\npreprocess_variants = ['raw', 'lemmatize', 'stem']\npreprocessed_texts = {\n    method: [preprocess(text, method) for text in train_texts]\n    for method in preprocess_variants\n}\ntest_preprocessed = [preprocess(text, 'lemmatize') for text in test_texts]  # temporary\n","metadata":{"id":"058ad1a1","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:42:48.855625Z","iopub.execute_input":"2025-06-16T17:42:48.855881Z","iopub.status.idle":"2025-06-16T17:42:50.636458Z","shell.execute_reply.started":"2025-06-16T17:42:48.855860Z","shell.execute_reply":"2025-06-16T17:42:50.635564Z"}},"outputs":[],"execution_count":24},{"id":"7ca9a240","cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nresults = {}\n\nfor method in preprocess_variants:\n    vectorizer = CountVectorizer()\n    X_train = vectorizer.fit_transform(preprocessed_texts[method])\n    X_test = vectorizer.transform([preprocess(text, method) for text in test_texts])\n\n    model = LogisticRegression(max_iter=1000)\n    model.fit(X_train, train_labels)\n    pred = model.predict(X_test)\n    acc = accuracy_score(test_labels, pred)\n    results[method] = acc\n\nprint(\"Preprocessing Accuracies (BoW + Logistic Regression):\")\nprint(pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy']))\n","metadata":{"id":"7ca9a240","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:42:50.638749Z","iopub.execute_input":"2025-06-16T17:42:50.639016Z","iopub.status.idle":"2025-06-16T17:42:57.313238Z","shell.execute_reply.started":"2025-06-16T17:42:50.638995Z","shell.execute_reply":"2025-06-16T17:42:57.312458Z"}},"outputs":[{"name":"stdout","text":"Preprocessing Accuracies (BoW + Logistic Regression):\n           Accuracy\nraw           0.752\nlemmatize     0.756\nstem          0.756\n","output_type":"stream"}],"execution_count":25},{"id":"29fed7e5","cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizers = {\n    \"BoW\": CountVectorizer(),\n    \"TF-IDF\": TfidfVectorizer()\n}\n\nX = {}\nfor name, vec in vectorizers.items():\n    X[name] = vec.fit_transform(preprocessed_texts['lemmatize'])\n    X[f\"{name}_test\"] = vec.transform(test_preprocessed)\n","metadata":{"id":"29fed7e5","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:42:57.313878Z","iopub.execute_input":"2025-06-16T17:42:57.314117Z","iopub.status.idle":"2025-06-16T17:42:57.467699Z","shell.execute_reply.started":"2025-06-16T17:42:57.314096Z","shell.execute_reply":"2025-06-16T17:42:57.466581Z"}},"outputs":[],"execution_count":26},{"id":"08adb923","cell_type":"code","source":"import gensim.downloader as api\n\nglove = api.load(\"glove-wiki-gigaword-100\")  # 100-dim\n\ndef embed_glove(texts):\n    embedded = []\n    for text in texts:\n        words = text.split()\n        vecs = [glove[word] for word in words if word in glove]\n        if vecs:\n            embedded.append(np.mean(vecs, axis=0))\n        else:\n            embedded.append(np.zeros(100))\n    return np.array(embedded)\n\nX[\"GloVe\"] = embed_glove(preprocessed_texts['lemmatize'])\nX[\"GloVe_test\"] = embed_glove(test_preprocessed)\n","metadata":{"id":"08adb923","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:42:57.468498Z","iopub.execute_input":"2025-06-16T17:42:57.468733Z","iopub.status.idle":"2025-06-16T17:43:29.486715Z","shell.execute_reply.started":"2025-06-16T17:42:57.468715Z","shell.execute_reply":"2025-06-16T17:43:29.485825Z"}},"outputs":[],"execution_count":27},{"id":"b1ee3215","cell_type":"code","source":"from gensim.models import Word2Vec\n\ntokenized = [text.split() for text in preprocessed_texts['lemmatize']]\nw2v_model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=1, workers=4)\n\ndef embed_w2v(texts):\n    return np.array([\n        np.mean([w2v_model.wv[word] for word in text.split() if word in w2v_model.wv] or [np.zeros(100)], axis=0)\n        for text in texts\n    ])\n\nX[\"Word2Vec\"] = embed_w2v(preprocessed_texts['lemmatize'])\nX[\"Word2Vec_test\"] = embed_w2v(test_preprocessed)\n","metadata":{"id":"b1ee3215","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:43:29.487926Z","iopub.execute_input":"2025-06-16T17:43:29.488227Z","iopub.status.idle":"2025-06-16T17:43:29.995358Z","shell.execute_reply.started":"2025-06-16T17:43:29.488203Z","shell.execute_reply":"2025-06-16T17:43:29.994261Z"}},"outputs":[],"execution_count":28},{"id":"8a7aca5c","cell_type":"code","source":"from gensim.models.fasttext import FastText\n\nft_model = FastText(sentences=tokenized, vector_size=100, window=3, min_count=1, workers=4)\n\ndef embed_fasttext(texts):\n    return np.array([\n        np.mean([ft_model.wv[word] for word in text.split() if word in ft_model.wv] or [np.zeros(100)], axis=0)\n        for text in texts\n    ])\n\nX[\"FastText\"] = embed_fasttext(preprocessed_texts['lemmatize'])\nX[\"FastText_test\"] = embed_fasttext(test_preprocessed)\n","metadata":{"id":"8a7aca5c","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:43:29.996774Z","iopub.execute_input":"2025-06-16T17:43:29.997133Z","iopub.status.idle":"2025-06-16T17:43:32.919977Z","shell.execute_reply.started":"2025-06-16T17:43:29.997109Z","shell.execute_reply":"2025-06-16T17:43:32.919235Z"}},"outputs":[],"execution_count":29},{"id":"7f64de3f","cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nmodels = {\n    \"LogReg\": LogisticRegression(max_iter=1000),\n    \"SVM\": SVC(),\n    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300),\n    \"RF\": RandomForestClassifier()\n}\n\nfinal_results = []\n\nfor vec_name in [\"BoW\", \"TF-IDF\", \"GloVe\", \"Word2Vec\", \"FastText\"]:\n    X_train = X[vec_name]\n    X_test = X[f\"{vec_name}_test\"]\n\n    for model_name, clf in models.items():\n        clf.fit(X_train, train_labels)\n        pred = clf.predict(X_test)\n        acc = accuracy_score(test_labels, pred)\n        final_results.append({\n            \"Vectorizer\": vec_name,\n            \"Classifier\": model_name,\n            \"Accuracy\": acc\n        })\n\ndf_results = pd.DataFrame(final_results)\nprint(df_results.pivot(index=\"Vectorizer\", columns=\"Classifier\", values=\"Accuracy\").round(4))\n","metadata":{"id":"7f64de3f","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:43:32.921050Z","iopub.execute_input":"2025-06-16T17:43:32.921739Z","iopub.status.idle":"2025-06-16T17:45:57.650667Z","shell.execute_reply.started":"2025-06-16T17:43:32.921717Z","shell.execute_reply":"2025-06-16T17:45:57.649493Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Classifier  LogReg    MLP     RF    SVM\nVectorizer                             \nBoW          0.756  0.706  0.696  0.704\nFastText     0.336  0.408  0.464  0.382\nGloVe        0.604  0.626  0.632  0.748\nTF-IDF       0.746  0.678  0.728  0.728\nWord2Vec     0.212  0.516  0.516  0.420\n","output_type":"stream"}],"execution_count":30}]}
