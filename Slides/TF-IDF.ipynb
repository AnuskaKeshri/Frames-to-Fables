{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce617c2e-f38f-48c9-888b-1c1d85da197e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Tutorial Using TF-IDF Vectorization\n",
    "\n",
    "This notebook shows how to perform sentiment analysis on text data using classical machine learning techniques with TF-IDF vectorization.  \n",
    "We will cover:\n",
    "\n",
    "- What TF-IDF is and why it’s useful  \n",
    "- Preprocessing text using **stemming** and **lemmatization**  \n",
    "- Vectorizing text with TF-IDF using unigrams and bigrams  \n",
    "- Training a Multinomial Naive Bayes classifier  \n",
    "- Comparing accuracy scores for different preprocessing and vectorization choices\n",
    "\n",
    "---\n",
    "\n",
    "## What is TF-IDF?\n",
    "\n",
    "**TF-IDF** stands for **Term Frequency - Inverse Document Frequency**. It is a numerical statistic that reflects how important a word is to a document in a collection (corpus).\n",
    "\n",
    "- **Term Frequency (TF):** Measures how often a term appears in a document.  \n",
    "- **Inverse Document Frequency (IDF):** Measures how unique or rare a term is across all documents. Words common across many documents get lower weight.\n",
    "\n",
    "Multiplying TF and IDF gives TF-IDF — a score that highlights words frequent in a document but rare in the whole corpus.\n",
    "\n",
    "**Why use TF-IDF?**\n",
    "\n",
    "- Unlike simple counts (Bag of Words), TF-IDF reduces the impact of very common words (like \"the\", \"is\") which carry less meaning.  \n",
    "- It better reflects the importance of words for classification or retrieval tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We have a small IMDB-like dataset of 10 sentences labeled as positive (1) or negative (0) sentiment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c15d34e-119b-47f4-a1f7-a26d4363d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901493de-827b-46a7-a78e-1d2f5b97a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Sample dataset\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I loved the movie, it was fantastic!\",\n",
    "        \"What a terrible film. I hated it.\",\n",
    "        \"An excellent movie with a great story.\",\n",
    "        \"Worst movie I have ever seen.\",\n",
    "        \"Absolutely wonderful experience, highly recommended!\",\n",
    "        \"Horrible acting and bad direction.\",\n",
    "        \"The plot was dull and boring.\",\n",
    "        \"A masterpiece. Beautifully made and touching.\",\n",
    "        \"Terrible! Do not waste your time.\",\n",
    "        \"Brilliant performance and amazing visuals.\"\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0, 0, 1, 0, 1]  # 1=Positive, 0=Negative\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79547a-3447-4912-805d-79d3baad3673",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "We need to clean and normalize the text before feeding it to our model.\n",
    "\n",
    "Steps include:\n",
    "\n",
    "- Tokenizing text into words  \n",
    "- Lowercasing  \n",
    "- Removing punctuation and stop words (common words like \"the\", \"is\" which add little meaning)  \n",
    "- Applying **stemming** (reducing words to their root form, e.g., \"loved\" → \"love\") or **lemmatization** (reducing words to dictionary base form)\n",
    "\n",
    "We will show outputs after each preprocessing step for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80423804-cd09-43f0-9a25-2133a4d59168",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, method='stem'):\n",
    "    tokens = word_tokenize(text.lower())  # tokenize and lowercase\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # remove punctuation and stop words\n",
    "    \n",
    "    if method == 'stem':\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    elif method == 'lemm':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395e0629-322f-46ae-aa21-275502193085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after Stemming:\n",
      "Sentence 1: ['love', 'movi', 'fantast']\n",
      "Sentence 2: ['terribl', 'film', 'hate']\n",
      "Sentence 3: ['excel', 'movi', 'great', 'stori']\n",
      "Sentence 4: ['worst', 'movi', 'ever', 'seen']\n",
      "Sentence 5: ['absolut', 'wonder', 'experi', 'highli', 'recommend']\n",
      "Sentence 6: ['horribl', 'act', 'bad', 'direct']\n",
      "Sentence 7: ['plot', 'dull', 'bore']\n",
      "Sentence 8: ['masterpiec', 'beauti', 'made', 'touch']\n",
      "Sentence 9: ['terribl', 'wast', 'time']\n",
      "Sentence 10: ['brilliant', 'perform', 'amaz', 'visual']\n"
     ]
    }
   ],
   "source": [
    "print(\"Output after Stemming:\")\n",
    "stemmed_texts = df['text'].apply(lambda x: preprocess_text(x, method='stem'))\n",
    "for i, tokens in enumerate(stemmed_texts):\n",
    "    print(f\"Sentence {i+1}:\", tokens)\n",
    "\n",
    "# Explanation:\n",
    "# Stemming cuts words down to their root form, often chopping off suffixes.\n",
    "# For example, 'loved' becomes 'love', 'fantastic' becomes 'fantast'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a987c19d-628c-494b-8eeb-50bb14c070ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output after Lemmatization:\n",
      "Sentence 1: ['loved', 'movie', 'fantastic']\n",
      "Sentence 2: ['terrible', 'film', 'hated']\n",
      "Sentence 3: ['excellent', 'movie', 'great', 'story']\n",
      "Sentence 4: ['worst', 'movie', 'ever', 'seen']\n",
      "Sentence 5: ['absolutely', 'wonderful', 'experience', 'highly', 'recommended']\n",
      "Sentence 6: ['horrible', 'acting', 'bad', 'direction']\n",
      "Sentence 7: ['plot', 'dull', 'boring']\n",
      "Sentence 8: ['masterpiece', 'beautifully', 'made', 'touching']\n",
      "Sentence 9: ['terrible', 'waste', 'time']\n",
      "Sentence 10: ['brilliant', 'performance', 'amazing', 'visuals']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOutput after Lemmatization:\")\n",
    "lemm_texts = df['text'].apply(lambda x: preprocess_text(x, method='lemm'))\n",
    "for i, tokens in enumerate(lemm_texts):\n",
    "    print(f\"Sentence {i+1}:\", tokens)\n",
    "\n",
    "# Explanation:\n",
    "# Lemmatization reduces words to their dictionary base form (lemma).\n",
    "# It is usually more accurate than stemming but requires more linguistic knowledge.\n",
    "# E.g., 'loved' remains 'loved' here because lemmatizer guesses noun form by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46799b-d320-4f6c-b874-c1164b11f8c2",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "\n",
    "We now convert the processed text into numeric feature vectors using TF-IDF.\n",
    "\n",
    "We will compare:\n",
    "\n",
    "- **Unigram TF-IDF:** Only single words (1-grams) as features  \n",
    "- **Bigram TF-IDF:** Single words and pairs of consecutive words (1-grams + 2-grams)\n",
    "\n",
    "For each case, we will print:\n",
    "\n",
    "- The learned vocabulary (all features)  \n",
    "- The TF-IDF vector for the first sentence  \n",
    "- Explanation of what the vector values mean  \n",
    "- Train and test a Multinomial Naive Bayes classifier  \n",
    "- Report accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6658f3b4-44cc-44fb-82f2-307afe38b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_texts_str = stemmed_texts.apply(lambda x: ' '.join(x))\n",
    "lemm_texts_str = lemm_texts.apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c81711-7dc3-4a8d-a947-ff15b88edccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary (ngram_range=(1, 1)):\n",
      " ['absolut' 'act' 'amaz' 'bad' 'beauti' 'bore' 'brilliant' 'direct' 'dull'\n",
      " 'ever' 'excel' 'experi' 'fantast' 'film' 'great' 'hate' 'highli'\n",
      " 'horribl' 'love' 'made' 'masterpiec' 'movi' 'perform' 'plot' 'recommend'\n",
      " 'seen' 'stori' 'terribl' 'time' 'touch' 'visual' 'wast' 'wonder' 'worst']\n",
      "\n",
      "TF-IDF vector for first sentence:\n",
      " [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.62583988 0.         0.         0.         0.         0.\n",
      " 0.62583988 0.         0.         0.46545557 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "Explanation: Each value is the TF-IDF score for that feature (word or n-gram) in the first sentence.\n",
      "Higher values mean the term is more important in this document relative to the corpus.\n",
      "Train labels distribution: [3 4]\n",
      "Test labels distribution: [2 1]\n",
      "Accuracy on test set: 0.333\n",
      "\n",
      "Vocabulary (ngram_range=(1, 2)):\n",
      " ['absolut' 'absolut wonder' 'act' 'act bad' 'amaz' 'amaz visual' 'bad'\n",
      " 'bad direct' 'beauti' 'beauti made' 'bore' 'brilliant'\n",
      " 'brilliant perform' 'direct' 'dull' 'dull bore' 'ever' 'ever seen'\n",
      " 'excel' 'excel movi' 'experi' 'experi highli' 'fantast' 'film'\n",
      " 'film hate' 'great' 'great stori' 'hate' 'highli' 'highli recommend'\n",
      " 'horribl' 'horribl act' 'love' 'love movi' 'made' 'made touch'\n",
      " 'masterpiec' 'masterpiec beauti' 'movi' 'movi ever' 'movi fantast'\n",
      " 'movi great' 'perform' 'perform amaz' 'plot' 'plot dull' 'recommend'\n",
      " 'seen' 'stori' 'terribl' 'terribl film' 'terribl wast' 'time' 'touch'\n",
      " 'visual' 'wast' 'wast time' 'wonder' 'wonder experi' 'worst' 'worst movi']\n",
      "\n",
      "TF-IDF vector for first sentence:\n",
      " [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.46864588 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.46864588 0.46864588 0.         0.\n",
      " 0.         0.         0.34854576 0.         0.46864588 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n",
      "\n",
      "Explanation: Each value is the TF-IDF score for that feature (word or n-gram) in the first sentence.\n",
      "Higher values mean the term is more important in this document relative to the corpus.\n",
      "Train labels distribution: [3 4]\n",
      "Test labels distribution: [2 1]\n",
      "Accuracy on test set: 0.333\n",
      "\n",
      "Vocabulary (ngram_range=(1, 1)):\n",
      " ['absolutely' 'acting' 'amazing' 'bad' 'beautifully' 'boring' 'brilliant'\n",
      " 'direction' 'dull' 'ever' 'excellent' 'experience' 'fantastic' 'film'\n",
      " 'great' 'hated' 'highly' 'horrible' 'loved' 'made' 'masterpiece' 'movie'\n",
      " 'performance' 'plot' 'recommended' 'seen' 'story' 'terrible' 'time'\n",
      " 'touching' 'visuals' 'waste' 'wonderful' 'worst']\n",
      "\n",
      "TF-IDF vector for first sentence:\n",
      " [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.62583988 0.         0.         0.         0.         0.\n",
      " 0.62583988 0.         0.         0.46545557 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "Explanation: Each value is the TF-IDF score for that feature (word or n-gram) in the first sentence.\n",
      "Higher values mean the term is more important in this document relative to the corpus.\n",
      "Train labels distribution: [3 4]\n",
      "Test labels distribution: [2 1]\n",
      "Accuracy on test set: 0.333\n",
      "\n",
      "Vocabulary (ngram_range=(1, 2)):\n",
      " ['absolutely' 'absolutely wonderful' 'acting' 'acting bad' 'amazing'\n",
      " 'amazing visuals' 'bad' 'bad direction' 'beautifully' 'beautifully made'\n",
      " 'boring' 'brilliant' 'brilliant performance' 'direction' 'dull'\n",
      " 'dull boring' 'ever' 'ever seen' 'excellent' 'excellent movie'\n",
      " 'experience' 'experience highly' 'fantastic' 'film' 'film hated' 'great'\n",
      " 'great story' 'hated' 'highly' 'highly recommended' 'horrible'\n",
      " 'horrible acting' 'loved' 'loved movie' 'made' 'made touching'\n",
      " 'masterpiece' 'masterpiece beautifully' 'movie' 'movie ever'\n",
      " 'movie fantastic' 'movie great' 'performance' 'performance amazing'\n",
      " 'plot' 'plot dull' 'recommended' 'seen' 'story' 'terrible'\n",
      " 'terrible film' 'terrible waste' 'time' 'touching' 'visuals' 'waste'\n",
      " 'waste time' 'wonderful' 'wonderful experience' 'worst' 'worst movie']\n",
      "\n",
      "TF-IDF vector for first sentence:\n",
      " [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.46864588 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.46864588 0.46864588 0.         0.\n",
      " 0.         0.         0.34854576 0.         0.46864588 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n",
      "\n",
      "Explanation: Each value is the TF-IDF score for that feature (word or n-gram) in the first sentence.\n",
      "Higher values mean the term is more important in this document relative to the corpus.\n",
      "Train labels distribution: [3 4]\n",
      "Test labels distribution: [2 1]\n",
      "Accuracy on test set: 0.333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate_logreg(texts, labels, ngram_range=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    y = labels\n",
    "    \n",
    "    print(f\"\\nVocabulary (ngram_range={ngram_range}):\\n\", vectorizer.get_feature_names_out())\n",
    "    print(f\"\\nTF-IDF vector for first sentence:\\n\", X.toarray()[0])\n",
    "    print(\"\\nExplanation: Each value is the TF-IDF score for that feature (word or n-gram) in the first sentence.\\n\"\n",
    "          \"Higher values mean the term is more important in this document relative to the corpus.\")\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    print(\"Train labels distribution:\", np.bincount(y_train))\n",
    "    print(\"Test labels distribution:\", np.bincount(y_test))\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy on test set: {acc:.3f}\")\n",
    "    return acc\n",
    "\n",
    "# Example usage with your processed data variables\n",
    "acc1 = train_and_evaluate_logreg(stemmed_texts_str, df['label'], ngram_range=(1,1))\n",
    "acc2 = train_and_evaluate_logreg(stemmed_texts_str, df['label'], ngram_range=(1,2))\n",
    "acc3 = train_and_evaluate_logreg(lemm_texts_str, df['label'], ngram_range=(1,1))\n",
    "acc4 = train_and_evaluate_logreg(lemm_texts_str, df['label'], ngram_range=(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfcbb8-8773-495a-bf68-94e2ee086f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
