{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8d0cea-c023-4528-9307-0c9403d4b49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "docs = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(docs)\n",
    "\n",
    "X_raw = [\" \".join(words) for words, label in docs]\n",
    "y = [1 if label == 'pos' else 0 for words, label in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e8be59-0733-4551-bfb5-fe54913cc233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews           : 2000\n",
      " - Positive Reviews     : 1000\n",
      " - Negative Reviews     : 1000\n",
      "Total Word Tokens       : 1583820\n",
      "Average Tokens/Review   : 791.91\n"
     ]
    }
   ],
   "source": [
    "num_reviews = len(docs)\n",
    "num_pos = sum(1 for _, label in docs if label == 'pos')\n",
    "num_neg = sum(1 for _, label in docs if label == 'neg')\n",
    "total_tokens = sum(len(words) for words, _ in docs)\n",
    "avg_tokens = total_tokens / num_reviews\n",
    "\n",
    "print(f\"Total Reviews           : {num_reviews}\")\n",
    "print(f\" - Positive Reviews     : {num_pos}\")\n",
    "print(f\" - Negative Reviews     : {num_neg}\")\n",
    "print(f\"Total Word Tokens       : {total_tokens}\")\n",
    "print(f\"Average Tokens/Review   : {avg_tokens:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36114ea-665a-4aea-817d-6a5d01af3d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : Positive\n",
      "Text  :\n",
      "one fun activity for parents during the holidays is to suggest an old film and see if they can interest their kids . although black - and - white films are frequently viewed as suspect , ones in color are greeted with more of an open mind . and if you can find a colorful action film , even if it is from six decades ago , then there is a real possibility of a take home hit . so it was in our family when we wandered over to the classic section of our local video store the other day and picked up a copy of the adventures of robin hood , a high spirited version of the walter scott story . nominated for the 1938 academy award for best picture and winner of three oscars for erich wolfgang korngold ' s melodramatic music , ralph dawson ' s fast paced editing and carl jules weyl ' s lush sets , the film is probably best remembered for errol flynn ' s charismatic acting as sir robin of locksley , a . k . a . robin hood . flynn , with his handsome figure and toothy smile , charms the audience wh...\n"
     ]
    }
   ],
   "source": [
    "sample_review_words, sample_label = docs[0]\n",
    "sample_text = \" \".join(sample_review_words)\n",
    "print(f\"Label : {'Positive' if sample_label == 'pos' else 'Negative'}\")\n",
    "print(\"Text  :\")\n",
    "print(sample_text[:1000] + (\"...\" if len(sample_text) > 1000 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dbc3ae-15a8-49cc-908c-fa17bb4c6a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001662b6-3e9b-4ed7-ad92-a90e4799419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e32ff54-bcfb-4c23-87b0-b6c93d0e9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentence_vector(tokens, embeddings, dim=100):\n",
    "    vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc49581-81e5-4e51-aab7-8d8f307d3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Accuracy: 0.635\n",
      "SkipGram Accuracy: 0.7266666666666667\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_tokens = [preprocess(text) for text in X_raw]\n",
    "\n",
    "w2v_cbow = Word2Vec(sentences=X_tokens, vector_size=100, window=5, sg=0, min_count=2)\n",
    "w2v_skipgram = Word2Vec(sentences=X_tokens, vector_size=100, window=5, sg=1, min_count=2)\n",
    "\n",
    "X_cbow = np.array([sentence_vector(tokens, w2v_cbow.wv, 100) for tokens in X_tokens])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cbow, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_cbow = LogisticRegression(max_iter=1000)\n",
    "clf_cbow.fit(X_train, y_train)\n",
    "print(\"CBOW Accuracy:\", accuracy_score(y_test, clf_cbow.predict(X_test)))\n",
    "\n",
    "X_skip = np.array([sentence_vector(tokens, w2v_skipgram.wv, 100) for tokens in X_tokens])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_skip, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_skip = LogisticRegression(max_iter=1000)\n",
    "clf_skip.fit(X_train, y_train)\n",
    "print(\"SkipGram Accuracy:\", accuracy_score(y_test, clf_skip.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deab09a9-6989-49d6-89f0-4dea56f44e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Accuracy: 0.7616666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(path=r\"C:\\Users\\ARITRA\\Downloads\\glove.6B (1)\\glove.6B.100d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vec = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vec\n",
    "    return embeddings\n",
    "\n",
    "glove = load_glove_embeddings()\n",
    "\n",
    "X_glove = np.array([sentence_vector(tokens, glove, 100) for tokens in X_tokens])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_glove, y, test_size=0.3, random_state=42)\n",
    "clf_glove = LogisticRegression(max_iter=1000)\n",
    "clf_glove.fit(X_train, y_train)\n",
    "print(\"GloVe Accuracy:\", accuracy_score(y_test, clf_glove.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a72adf7-4c39-4efd-8c8f-b5bffe399b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_bow = CountVectorizer(stop_words='english')\n",
    "X_bow = vectorizer_bow.fit_transform(X_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.3, random_state=42)\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"BoW Accuracy:\", accuracy_score(y_test, clf_bow.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41a0677a-57e8-4d08-acf3-7e2e61b792e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(X_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, clf_tfidf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ade63b-9bcd-43ec-883f-e94e0bf82a22",
   "metadata": {},
   "source": [
    "Interesting thing: Here we have averaged all the word vectors to represent each review - this can probably explain why WordtoVec and GloVE give worse accuracies than BoW and TF-IDF. But there are several other better ways, which you will (hopefully) explore in the upcoming assignment. Some of these ways are - \n",
    "\n",
    "i) Max Pooling\n",
    "\n",
    "ii) Weighted averaging (TF-IDF weighted)\n",
    "\n",
    "iii) Doc2Vec\n",
    "\n",
    "iv) RNNS and LSTMS on these word embeddings (in these the final hidden state is generally used for the representing each document (review in our case)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdb8af-aec5-48b2-b523-83501b11645c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
