{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ec2fac-4c6e-4986-925d-860403b44e8e",
   "metadata": {},
   "source": [
    "# Basic NLP Tutorial: Sentiment Analysis with Preprocessing and Bag of Words\n",
    "\n",
    "This tutorial will walk you through:\n",
    "- Text preprocessing: tokenization, stopword removal, stemming, and lemmatization\n",
    "- Vectorization using Bag of Words (unigrams) and n-grams (bigrams)\n",
    "- Applying classical ML algorithm (Multinomial Naive Bayes) for sentiment classification\n",
    "- Comparing accuracies and understanding the impact of preprocessing and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20672bfb-0d85-4819-abaa-f5989c2e8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ARITRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f217d3-258f-49ef-a0b7-7b565f6aeaa8",
   "metadata": {},
   "source": [
    "## Sample Dataset\n",
    "\n",
    "We'll use a tiny dataset similar to IMDB movie reviews, labeled with positive (1) or negative (0) sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4308360b-4905-48c6-9b02-a0563fdfea36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I loved the movie, it was fantastic!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a terrible film. I hated it.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An excellent movie with a great story.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Worst movie I have ever seen.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely wonderful experience, highly recomm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0               I loved the movie, it was fantastic!      1\n",
       "1                  What a terrible film. I hated it.      0\n",
       "2             An excellent movie with a great story.      1\n",
       "3                      Worst movie I have ever seen.      0\n",
       "4  Absolutely wonderful experience, highly recomm...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"text\": [\n",
    "        \"I loved the movie, it was fantastic!\",\n",
    "        \"What a terrible film. I hated it.\",\n",
    "        \"An excellent movie with a great story.\",\n",
    "        \"Worst movie I have ever seen.\",\n",
    "        \"Absolutely wonderful experience, highly recommended!\",\n",
    "        \"Horrible acting and bad direction.\",\n",
    "        \"The plot was dull and boring.\",\n",
    "        \"A masterpiece. Beautifully made and touching.\",\n",
    "        \"Terrible! Do not waste your time.\",\n",
    "        \"Brilliant performance and amazing visuals.\"\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0, 0, 1, 0, 1]  # 1 = Positive, 0 = Negative\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b7c336-ada1-4788-a047-8900b7fd4929",
   "metadata": {},
   "source": [
    "## Preprocessing Setup\n",
    "\n",
    "We will:\n",
    "- tokenize text into words\n",
    "- convert to lowercase\n",
    "- remove stopwords (common words like \"the\", \"is\", \"and\")\n",
    "- either stem or lemmatize words\n",
    "\n",
    "Let's define a preprocessing function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944f426e-7b70-45ba-b097-cf3bf786a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, method='stem'):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    if method == 'stem':\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    elif method == 'lemm':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca2633-89c9-452b-80c5-009a68b146c4",
   "metadata": {},
   "source": [
    "# Step 1: Stemming Output\n",
    "\n",
    "Let's preprocess the text using stemming and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cbb66ab-c03a-4cee-a46a-8128f8bb7cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. love movi fantast\n",
      "2. terribl film hate\n",
      "3. excel movi great stori\n",
      "4. worst movi ever seen\n",
      "5. absolut wonder experi highli recommend\n",
      "6. horribl act bad direct\n",
      "7. plot dull bore\n",
      "8. masterpiec beauti made touch\n",
      "9. terribl wast time\n",
      "10. brilliant perform amaz visual\n"
     ]
    }
   ],
   "source": [
    "stemmed_texts = df['text'].apply(lambda x: preprocess_text(x, method='stem'))\n",
    "for i, sent in enumerate(stemmed_texts, 1):\n",
    "    print(f\"{i}. {sent}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51d562-ad9b-4ce9-bb6f-762b9156bc8c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Words are reduced to their root forms using Porter Stemmer.\n",
    "- For example, \"loved\" → \"love\", \"fantastic\" → \"fantast\", \"horrible\" → \"horribl\".\n",
    "- This chopping sometimes creates non-real words but reduces vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d50d9-fdf9-4b4b-a532-0a909be00d62",
   "metadata": {},
   "source": [
    "# Step 2: Lemmatization Output\n",
    "\n",
    "Now let's preprocess using lemmatization instead, which returns valid base forms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f5a632-798d-4096-8d0f-729c61262890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. loved movie fantastic\n",
      "2. terrible film hated\n",
      "3. excellent movie great story\n",
      "4. worst movie ever seen\n",
      "5. absolutely wonderful experience highly recommended\n",
      "6. horrible acting bad direction\n",
      "7. plot dull boring\n",
      "8. masterpiece beautifully made touching\n",
      "9. terrible waste time\n",
      "10. brilliant performance amazing visuals\n"
     ]
    }
   ],
   "source": [
    "lemm_texts = df['text'].apply(lambda x: preprocess_text(x, method='lemm'))\n",
    "for i, sent in enumerate(lemm_texts, 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07e844-cc72-4522-8ab4-6ea9421e9f5b",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Lemmatization considers word meaning and context.\n",
    "- It maps \"loved\" → \"love\", \"movies\" → \"movie\", but keeps \"recommended\" as is.\n",
    "- It preserves more natural word forms compared to stemming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93913d50-9e65-4d19-ac47-6512e0df49a5",
   "metadata": {},
   "source": [
    "# Step 3A: Bag of Words (Unigram) Vectorization on Lemmatized Text\n",
    "\n",
    "Let's create a vocabulary of all words and vectorize the sentences with unigram counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b8da7d-a730-4ab0-ae08-0a0e337592ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (unigram) sample:\n",
      "['absolutely' 'acting' 'amazing' 'bad' 'beautifully' 'boring' 'brilliant'\n",
      " 'direction' 'dull' 'ever' 'excellent' 'experience' 'fantastic' 'film'\n",
      " 'great'] ...\n",
      "\n",
      "First sentence after lemmatization:\n",
      "\"loved movie fantastic\"\n",
      "Vector representation (counts) of first sentence:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_uni = CountVectorizer(ngram_range=(1,1))\n",
    "X_uni = vectorizer_uni.fit_transform(lemm_texts)\n",
    "vocab_uni = vectorizer_uni.get_feature_names_out()\n",
    "print(f\"Vocabulary (unigram) sample:\\n{vocab_uni[:15]} ...\\n\")\n",
    "\n",
    "print(\"First sentence after lemmatization:\")\n",
    "print(f\"\\\"{lemm_texts[0]}\\\"\")\n",
    "\n",
    "vector_0_uni = X_uni.toarray()[0]\n",
    "print(\"Vector representation (counts) of first sentence:\")\n",
    "print(vector_0_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda0411-ef7e-4549-b348-281eee535445",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Vocabulary lists unique words after preprocessing.\n",
    "- The vector for a sentence counts how many times each word appears.\n",
    "- In the first sentence, words like 'love', 'movie', and 'fantastic' have count=1.\n",
    "- Other vocab positions are zero since those words don't appear in this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a0a4f-508a-4bcf-b3cf-b8241be01e33",
   "metadata": {},
   "source": [
    "# Step 3B: Bigram (Unigrams + Bigrams) Vectorization on Lemmatized Text\n",
    "\n",
    "Now let's vectorize using both unigrams and bigrams (pairs of consecutive words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5924036-8648-4cbc-b996-0ed0893a1d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (unigrams + bigrams) sample:\n",
      "['absolutely' 'absolutely wonderful' 'acting' 'acting bad' 'amazing'\n",
      " 'amazing visuals' 'bad' 'bad direction' 'beautifully' 'beautifully made'\n",
      " 'boring' 'brilliant' 'brilliant performance' 'direction' 'dull'\n",
      " 'dull boring' 'ever' 'ever seen' 'excellent' 'excellent movie'] ...\n",
      "\n",
      "First sentence after lemmatization:\n",
      "\"loved movie fantastic\"\n",
      "Vector representation (counts) of first sentence with bigrams:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_bi = CountVectorizer(ngram_range=(1,2))\n",
    "X_bi = vectorizer_bi.fit_transform(lemm_texts)\n",
    "vocab_bi = vectorizer_bi.get_feature_names_out()\n",
    "print(f\"Vocabulary (unigrams + bigrams) sample:\\n{vocab_bi[:20]} ...\\n\")\n",
    "\n",
    "print(\"First sentence after lemmatization:\")\n",
    "print(f\"\\\"{lemm_texts[0]}\\\"\")\n",
    "\n",
    "vector_0_bi = X_bi.toarray()[0]\n",
    "print(\"Vector representation (counts) of first sentence with bigrams:\")\n",
    "print(vector_0_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991268ae-c7a0-4235-a666-4fa60d100c52",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Vocabulary now includes single words *and* two-word sequences (bigrams).\n",
    "- The vector counts appearances of unigrams and bigrams.\n",
    "- For example, the bigrams 'love movie' and 'movie fantastic' appear once each in the first sentence.\n",
    "- This adds phrase-level context beyond single words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29169a9-6002-4dc2-b5cd-6c5c0e2c0944",
   "metadata": {},
   "source": [
    "# Step 4: Model Training and Accuracy Comparison\n",
    "\n",
    "Let's train Multinomial Naive Bayes models on all four setups and compare accuracies:\n",
    "1. Stem + Unigram\n",
    "2. Stem + Bigram\n",
    "3. Lemmatize + Unigram\n",
    "4. Lemmatize + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753ccd73-2ef9-4fe8-88ce-55053b2b37e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using stem + ngram 1-1: 0.3333\n",
      "Accuracy using stem + ngram 1-2: 1.0000\n",
      "Accuracy using lemm + ngram 1-1: 0.3333\n",
      "Accuracy using lemm + ngram 1-2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for method in ['stem', 'lemm']:\n",
    "    processed_texts = df['text'].apply(lambda x: preprocess_text(x, method=method))\n",
    "    for ngram in [(1, 1), (1, 2)]:\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y = df['label']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        model = MultinomialNB()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        results.append((method, ngram, acc))\n",
    "\n",
    "for method, ngram, acc in results:\n",
    "    ngram_str = f\"{ngram[0]}-{ngram[1]}\"\n",
    "    print(f\"Accuracy using {method} + ngram {ngram_str}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab101a57-7bb2-4a54-aad6-ac4da329cd59",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Lemmatization generally yields better accuracy because it keeps words meaningful.\n",
    "- Adding bigrams sometimes helps by capturing common phrases, but difference is often small on tiny datasets.\n",
    "- Stemming may harm accuracy due to aggressive chopping creating non-real words.\n",
    "\n",
    "This illustrates how preprocessing and feature extraction choices impact NLP model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb577f-bf3f-45ef-a541-a7e76b2843a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
